\documentclass[10pt,a4paper,english]{article}

% be quiet
\batchmode

% front matter%FOLDUP
\usepackage[hyphens]{url}
\usepackage{amsmath}
\usepackage{amsfonts}
% for therefore
\usepackage{amssymb}
% for theorems?
\usepackage{amsthm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}{Example}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{caution}{Caution}
\newtheorem*{note}{Note}

% see http://tex.stackexchange.com/a/3034/2530
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{hyperref}
\usepackage[square,numbers]{natbib}
%\usepackage[authoryear]{natbib}
%\usepackage[iso]{datetime}
%\usepackage{datetime}
\usepackage{gitinfo2}

%%http://choorucode.com/2010/05/05/how-to-add-draft-watermark-in-latex/
%\usepackage{draftwatermark}
%% V1 sent to Mortada.
%% V2 on paper to JHZD.
%% V3 sent to s.lee@BR 140826
%\providecommand{\versnum}{V4}
%\SetWatermarkText{DRAFT \versnum}
%\SetWatermarkLightness{0.87}
%\SetWatermarkScale{4.5}

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\chead{}
%\rhead{}
%\lhead{}
%\rhead{\sc draft \versnum; do not distribute}
%\rfoot{}

%compactitem and such:
\usepackage[newitem,newenum,increaseonly]{paralist}

\makeatletter
\makeatother

% see https://www.overleaf.com/blog/619-tip-of-the-week-add-inline-or-margin-comments-to-your-pdf#.Wk26SN-YWXI
\usepackage[colorinlistoftodos]{todonotes}
% for release, disable them!
%\usepackage[disable]{todonotes}

%\input{sr_defs.tex}
\usepackage[notheorems]{SharpeR}

\providecommand{\sideWarning}[1][0.5]{\marginpar{\hfill\includegraphics[width=#1\marginparwidth]{warning}}}

% transformed... 
\providecommand{\altSNR}[0]{\xi}
\providecommand{\ptvsnr}[1][]{\vectUL{\altSNR}{}{#1}}
\providecommand{\stvsr}[1][]{\vectUL{\hat{\altSNR}}{}{#1}}
\providecommand{\ptsrUL}[2]{\mathUL{\altSNR}{#1}{#2}}
\providecommand{\ptsnr}[1][]{\ptsrUL{}{#1}}
\providecommand{\stsrUL}[2]{\mathUL{\hat{\altSNR}}{#1}{#2}}
\providecommand{\stsr}[1][]{\stsrUL{}{#1}}
\providecommand{\ssravg}[1][]{\mathUL{\bar{\prvSNR}}{}{#1}}
%\providecommand{\logof}[1]{\funcit{\operatorname{log}}{#1}}  
%\providecommand{\minof}[1]{\funcit{\operatorname{min}}{#1}}  

% knitr setup%FOLDUP

<<'preamble', include=FALSE, warning=FALSE, message=FALSE>>=
library(knitr)

# set the knitr options ... for everyone!
# if you unset this, then vignette build bonks. oh, joy.
#opts_knit$set(progress=TRUE)
opts_knit$set(eval.after='fig.cap')
# for a package vignette, you do want to echo.
# opts_chunk$set(echo=FALSE,warning=FALSE,message=FALSE)
opts_chunk$set(warning=FALSE,message=FALSE)
#opts_chunk$set(results="asis")
mybit <- 'maxsharpe_'
opts_chunk$set(cache=TRUE,cache.path=file.path("cache",mybit))

#opts_chunk$set(fig.path="figure/",dev=c("pdf","cairo_ps"))
opts_chunk$set(fig.path=file.path("figure",mybit),dev=c("pdf"))
opts_chunk$set(fig.width=5,fig.height=4,dpi=64)

# doing this means that png files are made of figures;
# the savings is small, and it looks like shit:
#opts_chunk$set(fig.path="figure/",dev=c("png","pdf","cairo_ps"))
#opts_chunk$set(fig.width=4,fig.height=4)
# for figures? this is sweave-specific?
#opts_knit$set(eps=TRUE)

# this would be for figures:
#opts_chunk$set(out.width='.8\\textwidth')
# for text wrapping:
options(width=64,digits=3)
opts_chunk$set(size="small")
opts_chunk$set(tidy=TRUE,tidy.opts=list(width.cutoff=50,keep.blank.line=TRUE))

compile.time <- Sys.time()

# from the environment

# only recompute if FORCE_RECOMPUTE=True w/out case match.
FORCE_RECOMPUTE <- (toupper(Sys.getenv('FORCE_RECOMPUTE',unset='False')) == "TRUE")
@
%UNFOLD

<<'runtime', include=FALSE, warning=FALSE, message=FALSE, cache=TRUE>>=
# compiler flags!

# you can manipulate the 'runtime' by setting this variable.
# by convention the value 1 should take the longest time,
# while larger values should result in quicker execution
# (for low fidelity simulations)
# takes the value from the environment variable of the same name,
# or 1 if missing.

RUNTIME_PARAM <- max(1,as.numeric(Sys.getenv('RUNTIME_PARAM')),na.rm=TRUE)
@
<<'plot_funcs',include=FALSE>>=
suppressMessages({
	library(dplyr)
  library(ggplot2)
})

modqq <- function(sample,qnt_func=qnorm,dens_func=dnorm,k=9,ppp=ppoints(k)) {
  require(dplyr,quietly=TRUE)
  nnn <- length(sample)
  xi_true <- qnt_func(ppp)
  xi_hat  <- quantile(sample,ppp)
  sigma2  <- ppp * (1-ppp) / (dens_func(xi_true)^2)
  serr <- sqrt(sigma2 / nnn)
  retv <- data_frame(ppp=ppp,xi=xi_true,xi_hat=xi_hat,serr=serr) %>%
    mutate(delta = xi_hat - xi) %>%
    arrange(ppp) %>%
    mutate(iii=seq_len(n()))
  attr(retv,'nnn') <- nnn
  retv
}

# this it the log of U which is uniform on [min,max]
dlogunif <- function(x,min=0,max=1) {
  diffy <- max - min
  dens <- rep(0,length(x))
  inrange <- (log(min) <= x) & (x <= log(max))
  dens[inrange] <- 1 / (diffy * x[inrange])
  dens
}
qlogunif <- function(p,min=0,max=1,lower.tail=TRUE,log.p=FALSE) { 
  log(qunif(p,min=min,max=max,lower.tail=lower.tail,log.p=log.p))
}
# cdf & qtile, max of np independent SRs, SNR=zeta
pmaxsr <- function(q,df,np,zeta=0,ope=1) {
	pv <- SharpeR::psr(q,df=df,zeta=zeta,ope=ope)
	pbeta(pv,shape1=np,shape2=1)
}
qmaxsr <- function(p,df,np,zeta=0,ope=1) {
	pv <- qbeta(p,shape1=np,shape2=1)
	SharpeR::qsr(pv,df=df,zeta=zeta,ope=ope)
}

# try a different kind of plot
cdf_helper <- function(x,qts=c(0.001,0.005,0.01,0.05,0.1),alpha=0.05) {
  if (!require(zipper) && (require(devtools))) {
    devtools::install_github("shabbychef/zipper")
  }
  x <- x[!is.na(x)]
  nnn <- length(x)
  emp_cdf <- zipper::zip_le(sort(x),qts) / nnn
  nn_lo <- qbinom(alpha/2,size=nnn,prob=qts,lower.tail=TRUE) 
  nn_hi <- qbinom(alpha/2,size=nnn,prob=qts,lower.tail=FALSE)
  qt_lo <- pbinom(nn_lo,size=nnn,prob=qts,lower.tail=TRUE)
  qt_hi <- pbinom(nn_lo,size=nnn,prob=qts,lower.tail=FALSE)
  retv <- data.frame(qts=qts,emp_cdf=emp_cdf,
                     ci_lo=nn_lo / nnn,ci_hi=nn_hi / nnn,
                     qt_lo=qt_lo,qt_hi=qt_hi)
  retv$width <- retv$qt_hi - retv$qt_lo
  attr(retv,'alpha') <- alpha
  retv
}
@
    
% SYMPY preamble%FOLDUP
    
    %\usepackage{graphicx} % Used to insert images
    %\usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{color} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    %\usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    %\usepackage[utf8]{inputenc} % Allow utf-8 characters in the tex document
    %\usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
		\usepackage{fancyvrb} % verbatim replacement that allows latex
    %\usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    %\usepackage{longtable} % longtable support required by pandoc >1.10
    

    
    
    \definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
    \definecolor{darkorange}{rgb}{.71,0.21,0.01}
    \definecolor{darkgreen}{rgb}{.12,.54,.11}
    \definecolor{myteal}{rgb}{.26, .44, .56}
    \definecolor{gray}{gray}{0.45}
    \definecolor{lightgray}{gray}{.95}
    \definecolor{mediumgray}{gray}{.8}
    \definecolor{inputbackground}{rgb}{.95, .95, .85}
    \definecolor{outputbackground}{rgb}{.95, .95, .95}
    \definecolor{traceback}{rgb}{1, .95, .95}
    % ansi colors
    \definecolor{red}{rgb}{.6,0,0}
    \definecolor{green}{rgb}{0,.65,0}
    \definecolor{brown}{rgb}{0.6,0.6,0}
    \definecolor{blue}{rgb}{0,.145,.698}
    \definecolor{purple}{rgb}{.698,.145,.698}
    \definecolor{cyan}{rgb}{0,.698,.698}
    \definecolor{lightgray}{gray}{0.5}
    
    % bright ansi colors
    \definecolor{darkgray}{gray}{0.25}
    \definecolor{lightred}{rgb}{1.0,0.39,0.28}
    \definecolor{lightgreen}{rgb}{0.48,0.99,0.0}
    \definecolor{lightblue}{rgb}{0.53,0.81,0.92}
    \definecolor{lightpurple}{rgb}{0.87,0.63,0.87}
    \definecolor{lightcyan}{rgb}{0.5,1.0,0.83}
    
    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    
    %\DefineShortVerb[commandchars=\\\{\}]{\|}
    %\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    %% Add ',fontsize=\small' for more characters per line
    %\newenvironment{Shaded}{}{}
    %\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    %\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    %\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    %\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    %\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    %\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    %\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    %\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    %\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    %\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    %\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    %\newcommand{\RegionMarkerTok}[1]{{#1}}
    %\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    %\newcommand{\NormalTok}[1]{{#1}}
    
    %% Define a nice break command that doesn't care if a line doesn't already
    %% exist.
    %\def\br{\hspace*{\fill} \\* }
    %% Math Jax compatability definitions
    %\def\gt{>}
    %\def\lt{<}
    

    %% Pygments definitions
    
%\makeatletter
%\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    %\let\PY@ul=\relax \let\PY@tc=\relax%
    %\let\PY@bc=\relax \let\PY@ff=\relax}
%\def\PY@tok#1{\csname PY@tok@#1\endcsname}
%\def\PY@toks#1+{\ifx\relax#1\empty\else%
    %\PY@tok{#1}\expandafter\PY@toks\fi}
%\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    %\PY@it{\PY@bf{\PY@ff{#1}}}}}}}
%\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

%\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
%\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
%\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
%\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
%\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
%\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
%\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
%\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
%\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
%\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
%\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
%\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
%\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
%\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
%\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
%\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
%\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
%\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
%\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
%\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
%\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
%\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
%\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
%\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
%\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
%\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
%\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
%\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
%\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
%\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
%\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
%\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
%\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
%\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
%\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
%\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
%\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
%\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
%\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
%\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
%\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
%\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
%\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
%\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
%\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
%\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
%\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
%\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
%\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
%\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
%\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
%\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
%\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
%\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
%\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
%\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
%\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
%\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
%\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
%\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

%\def\PYZbs{\char`\\}
%\def\PYZus{\char`\_}
%\def\PYZob{\char`\{}
%\def\PYZcb{\char`\}}
%\def\PYZca{\char`\^}
%\def\PYZam{\char`\&}
%\def\PYZlt{\char`\<}
%\def\PYZgt{\char`\>}
%\def\PYZsh{\char`\#}
%\def\PYZpc{\char`\%}
%\def\PYZdl{\char`\$}
%\def\PYZhy{\char`\-}
%\def\PYZsq{\char`\'}
%\def\PYZdq{\char`\"}
%\def\PYZti{\char`\~}
%% for compatibility with earlier versions
%\def\PYZat{@}
%\def\PYZlb{[}
%\def\PYZrb{]}
%\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=blue,
      linkcolor=darkorange,
      citecolor=darkgreen,
      }
    % Slightly bigger margins than the latex defaults
    
    %\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    %UNFOLD
%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% commands specific to this paper:%FOLDUP
\providecommand{\RMAT}[1][{}]{\MtxUL{R}{}{#1}}
\providecommand{\RHAT}[1][{}]{\MtxUL{\hat{R}}{}{#1}}
\providecommand{\AAA}[1][{}]{\MtxUL{A}{}{#1}}
\providecommand{\bbb}[1][{}]{\vectUL{b}{}{#1}}
\providecommand{\ccc}[1][{}]{\vectUL{c}{}{#1}}
\providecommand{\zzz}[1][{}]{\vectUL{z}{}{#1}}
\providecommand{\etav}[1][{}]{\vectUL{\eta}{}{#1}}
\providecommand{\yvec}[1][{}]{\vectUL{y}{}{#1}}
\providecommand{\Vfunc}[1]{\mathUL{\mathcal{V}}{#1}{}}
\providecommand{\Vmin}{\Vfunc{-}}
\providecommand{\Vmax}{\Vfunc{+}}
\providecommand{\tncdf}[5]{\funcit{F}{{#1};{#2},{#3},{#4},{#5}}}
\providecommand{\makerho}[3][{\vone}]{{#2}\wrapParens{\ogram{#1}} + {#3}\,\eye}
\providecommand{\pmone}[1][{}]{\vectUL{w}{}{#1}}
\providecommand{\posp}[1]{\mathUL{{#1}}{}{+}}
\providecommand{\pospsq}[1]{\mathUL{{#1}}{2}{+}}


\providecommand{\xxx}[1][{}]{\vectUL{x}{}{#1}}
\providecommand{\yyy}[1][{}]{\vectUL{y}{}{#1}}
\providecommand{\xxs}[1][{}]{\mathUL{x}{}{#1}}
\providecommand{\yys}[1][{}]{\mathUL{y}{}{#1}}

%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% document incantations%FOLDUP
\begin{document}

%\title{Conditional estimation on the \txtSNR of the asset with maximum \txtSR}
\title{Conditional inference on the asset with maximum Sharpe ratio}
\author{Steven E. Pav \thanks{\email{steven@gilgamath.com}.
The source code to build this document is available at
\href{http://www.github.com/shabbychef/maxsharpe}{\normalfont\texttt{www.github.com/shabbychef/maxsharpe}}.
The document you are reading was built from commit \texttt{\gitHash} of that repo.
}}
%The code to build this document is available at
%\href{http://www.github.com/shabbychef/qbound}{\normalfont\texttt{www.github.com/shabbychef/qbound}}.
%\date{\today, \currenttime}

\maketitle
%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}%FOLDUP
We apply the procedure of Lee \etal \cite{lee2013exact} to the problem of performing
inference on the \txtSNR of the asset which displays maximum sample \txtSR over
a set of possibly correlated assets.
We find a multivariate analogue of the commonly used
approximate standard error of the \txtSR to use in this conditional estimation procedure.
We also consider the simple Bonferroni correction for multiple
hypothesis testing, fixing it for the case of positive common
correlation among assets, and a chi-bar square test against one-sided alternatives.

Testing indicates the conditional inference procedure achieves nominal type I rate,
and does not appear to suffer from non-normality of returns.
The conditional estimation test has low power under the alternative
where there is little spread in the \txtSNRs of the assets, 
and high power under the alternative where a single asset has high \txtSNR.
\end{abstract}%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}%FOLDUP

The problem of overfitting quantitative investment strategies
is certainly as old as the problem of selecting quantitative investment
strategies.
The choice of a course of action (\eg making an investment) based on 
historical observations leads to biased estimates of 
the value of the selected course of action when one uses the same historical
observations to estimate value.
That is, the estimates are ``biased by selection''.
This problem is not unique to quantitative finance, and goes
by many names: overfitting, p-hacking, data-mining bias, \etc
To be clear we are interested in the case where one has
observed \ssiz independent contemporaneous observations of returns
from \nstrat different ``assets'' (these can be trading strategy
backtest returns, or mutual fund returns, \etc), selects
one of those assets based on the historical performance, say
by selecting the asset with maximum \txtSR; then one wishes
to estimate or perform inference on the true `value' of the asset, for
example its \txtSNR, which we define as population analogue of the
\txtSR.

Aronson gives a good overview of the problem from the practicioner's
point of view, noting the relevant factors are the length of
history, the number of strategies tested, the correlation of their
historical performance, presence of outliers (or fat-tailedness of returns), and variation in
expected true effect size.  \cite[Chapter 6]{Aronson2007}
White's Reality Check was a pioneering development in the
area, giving a generally applicable method of estimating 
whether a selected model was superior to a benchmark model. \cite{White:2000}
White's work was extended and generalized by Romano and Wolf, Hansen, \emph{inter alia}. 
\cite{romano2005stepwise,Hansen:2005,Hsu2010471}
From a practical point of view the Reality Check and its variants do not scale
computationally to hundreds or thousands of assets, as they are based on
a (block) bootstrap. However, these methods can be adapted to very general
problems, can deal with correlation and autocorrelation of asset returns,
and are fairly robust to assumptions.

Recent work by L\'{o}pez de Prado and Bailey,
adapting standard techniques from Multiple Hypothesis Testing (MHT),
has gained attention 
in the field\footnote{Though application of MHT corrections
to the problem is not new: White's starting
assumption was apparently that such simple corrections
were inadequate. \cite{White:2000}}.  \cite{lopezdeprado2018false} 
They find the asymptotic expected value of
the maximum \txtSR of uncorrelated assets with zero \txtSNR.  
While use of simple techniques from MHT (Bonferroni correction, say)
can lead to reduced power, and is fragile with respect to assumptions,
they are alluring in their simplicity.
The Bonferroni correction is very simple to describe and implement,
and does not require one to store the historical returns of the
assets. It easily scales to millions of tested assets.

In this paper we exploit a result from Lee \etal on the problem
of \emph{conditional estimation}.  \cite{lee2013exact}
The Lee procedure was originally devised for analysis of the Lasso,
but is applicable in general to the case of selection from a 
normally distributed vector conditional to a linear constraint.
We simply give an multivariate normal approximation to the
vector of \txtSRs of \nstrat assets, then appeal to the Lee
\etal procedure.  

Our procedure is midway between the simple MHT
correction and the Reality Check tests, both computationally
and in robustness. Our procedure requires one to estimate
the correlation between returns, which would appear to require
\bigo{\ssiz \nstrat^2} runtimes. However, only the correlation of the
selected asset against all others is required, reducing the burden to \bigo{\ssiz\nstrat}.
Unlike the bootstrap tests, our procedure is easily adapted
to the case of producing confidence intervals on the 
\txtSNR, instead of only supporting hypothesis testing.

%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conditional Inference on the \txtSNR}%FOLDUP

% introduction%FOLDUP
We consider the following problem: one has observed \ssiz \iid samples
of some $\nstrat$-vector \vreti, representing the returns of $\nstrat$
different ``assets,'' which could be stocks, trading strategies, \etc 
From the sample one computes the \txtSR of each asset, resulting in 
a $\nstrat$-vector, \svsr. One will then choose the asset with maximum
\txtSR. One then seeks to perform hypothesis tests or compute
confidence intervals on the \txtSNR of that asset. 
Here we define the \txtSR as the sample mean divided by sample
standard deviation, and the \txtSNR as the population analogue.
Throughout we use hats to denote sample quantities estimating
population parameters.

To simplify the exposition, we will suppose that, conditional 
on observing the vector \svsr, one rearranges the indices such
that the first asset has demonstrated the highest \txtSR.
This is to avoid the cumbersome notation of $\ssr[(1)]$,
and we instead can just write $\ssr[1]$. We note this maximum
condition can be written in the form $\AAA\svsr \le \bbb$ for
\bby{\nstrat-1}{\nstrat} matrix $\AAA$ defined by
$$
\AAA = \wrapBracks{\begin{array}{ccccc}
  -1 & 1 & 0 & \ldots & 0\\
  -1 & 0 & 1 & \ldots & 0\\
  \vdots & \vdots & \vdots & \ddots & \vdots\\
  -1 & 0 & 0 & \ldots & 1
\end{array}},
%\quad
%\bbb=\wrapBracks{\begin{array}{c}
  %0\\
  %0\\
  %\vdots\\
  %0
%\end{array}}.
$$
and where $\bbb$ is the $\wrapParens{\nstrat-1}$-dimensional zero vector.
Also note that we are interested in performing inference on $\psnr[1]$,
which we can express as $\trAB{\etav}{\pvsnr}$ for $\etav=\basev[1]$.

Under these conditions, if only $\svsr$ were normally distributed, one
could use the following theorem due to Lee \etal:
\begin{theorem}[Lee \etal, Theorem 5.2 \cite{lee2013exact}]
\label{theorem:lee_etal}
Suppose $\yvec\sim\normlaw{\pvmu,\pvsig}$. 
  Define 
  $\ccc=\pvsig\etav / \qform{\pvsig}{\etav},$ and
  $\zzz=\yvec - \ccc\trAB{\etav}{\yvec}.$
%
  Let \pnorm[x] be the CDF of a standard normal, and 
  let \tncdf{x}{a}{b}{0}{1} be the CDF of a standard normal truncated
  to $\ccinterval{a}{b}$:
  $$
  \tncdf{x}{a}{b}{0}{1}\defeq\frac{\pnorm[x]-\pnorm[a]}{\pnorm[b] - \pnorm[a]}.
  $$
  Let \tncdf{x}{a}{b}{\pmu}{\psigsq} be the CDF of a general truncated normal,
  defined by
  $$
  \tncdf{x}{a}{b}{\pmu}{\psigsq} =
  \tncdf{\frac{x-\pmu}{\psigma}}{\frac{a-\pmu}{\psigma}}{\frac{b-\pmu}{\psigma}}{0}{1}.
  $$
  Then, conditional on $\AAA\yvec\le\bbb$, the random variable
  $$
  \tncdf{\trAB{\etav}{\yvec}}{\Vmin}{\Vmax}{\trAB{\etav}{\pvmu}}{\qform{\pvsig}{\etav}}
  $$
  is Uniform on $\ccinterval{0}{1}$, where \Vmin and \Vmax are given by
  \begin{align*}
    \Vmin &= \max_{j:\wrapParens{\AAA\ccc}_j < 0} \frac{\bbb[j] -
    \wrapParens{\AAA\zzz}_j}{\wrapParens{\AAA\ccc}_j},\\
    \Vmax &= \min_{j:\wrapParens{\AAA\ccc}_j > 0} \frac{\bbb[j] -
    \wrapParens{\AAA\zzz}_j}{\wrapParens{\AAA\ccc}_j}.
  \end{align*}
\end{theorem}

This theorem gives us a way to perform hypothesis tests, 
by comparing 
$\tncdf{\trAB{\etav}{\yvec}}{\Vmin}{\Vmax}{\trAB{\etav}{\pvmu}}{\qform{\pvsig}{\etav}}$
to some cutoff.
It also suggests a procedure for computing confidence intervals on 
$\trAB{\etav}{\pvmu}$, namely by univariate search for a value of
$\trAB{\etav}{\pvmu}$ such that 
$\tncdf{\trAB{\etav}{\yvec}}{\Vmin}{\Vmax}{\trAB{\etav}{\pvmu}}{\qform{\pvsig}{\etav}}$
is equal to some cutoff value.

In the following section we will show that the \svsr is \emph{approximately}
normally distributed.
In the following section we will examine whether the normal approximation 
is good enough to use the procedure of Lee \etal for testing the 
\txtSNR of the asset with maximum \txtSR.
%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Normal approximation of the distribution of \txtSRs}%FOLDUP

Here we derive the asymptotic distribution of \txtSR, following
Jobson and Korkie 
\emph{inter alia}. \cite{jobsonkorkie1981,lo2002,mertens2002comments,pav_ssc}
Consider the case of \nstrat possibly correlated returns streams,
with each observation denoted by the $\nstrat$-vector \vreti.
Let \pvmu be the \nstrat-vector of population means, and let
\pvmom[2] be the \nstrat-vector of the uncentered second moments.
Let \pvsnr be the vector of \txtSNRs of the assets. Let \rfr be the
`risk free rate'. We have
$$
\pvsnr[i] = \frac{\pvmu[i] - \rfr}{\sqrt{\pvmom[2,i] - \pvmu[i]^2}}.
$$

Consider the $2\nstrat$ vector of \vreti, `stacked' with
\vreti squared elementwise, \vcat{\vreti}{\vreti^2}.
The expected value of this vector is \vcat{\pvmu}{\pvmom[2]};
let \pvvar be the variance of this vector, assuming it exists.

Given \ssiz observations of \vreti, consider the simple
sample estimate
$$
\vcat{\svmu}{\svmom[2]} \defeq
\frac{1}{\ssiz}\sum_{i}^{\ssiz} \vcat{\vreti}{\vreti^2}.
$$
Under the multivariate central limit theorem \cite{wasserman2004all}
\begin{equation}
\sqrt{\ssiz}\wrapParens{\vcat{\svmu}{\svmom[2]} - \vcat{\pvmu}{\pvmom[2]}}
\rightsquigarrow 
\normlaw{0,\pvvar}.
\label{eqn:mvclt}
\end{equation}

Let \svsr be the sample \txtSR computed from the estimates \svmu and
\svmom[2]: 
$\svsr[i] = \fracc{\wrapParens{\svmu[i]-\rfr}}{\sqrt{\svmom[2,i] - \svmu[i]^2}}.
$
By the multivariate delta method, 
\begin{equation}
\sqrt{\ssiz}\wrapParens{\svsr - \pvsnr} 
\rightsquigarrow 
\normlaw{0,\qoform{\pvvar}{\wrapParens{\dbyd{\pvsnr}{\vcat{\pvmu}{\pvmom[2]}}}}}.
\label{eqn:delmethod}
\end{equation}
Here the derivative takes the form of two \sbby{\nstrat}
diagonal matrices pasted together side by side:
\begin{equation}
\begin{split}
\dbyd{\pvsnr}{\vcat{\pvmu}{\pvmom[2]}} 
&= 
\onebytwo{\Mdiag{\frac{\pvmom[2] - \pvmu\rfr}{\wrapParens{\pvmom[2] - \pvmu^2}^{3/2}}}}{ 
\Mdiag{\frac{\rfr-\pvmu}{2 \wrapParens{\pvmom[2] - \pvmu^2}^{3/2}}}},\\
&=  
\onebytwo{\Mdiag{\frac{\pvsigma + \pvmu\pvsnr}{\pvsigma^2}}}
{\Mdiag{\frac{- \pvsnr}{2 \pvsigma^2}}}.
\end{split}
\label{eqn:sr_deriv}
\end{equation}
where $\Mdiag{\vect{z}}$ is the matrix with vector \vect{z} on its diagonal,
and where the vector operations above are all performed elementwise,
where we define the vector 
$\pvsigma \defeq \wrapParens{\pvmom[2] - \pvmu^2}^{1/2}$,
with powers taken elementwise.
%Thus \eqnref{mvclt} can be written as
%\begin{equation}
%\svsr \approx 
%%\normlaw{\pvsnr,\oneby{\ssiz}\qoform{\svvar}{\wrapParens{\dbyd{\svsr}{\vcat{\svmu}{\svmom[2]}}}}},
%\normlaw{\pvsnr,\oneby{\ssiz}
%\onebytwo{\Mdiag{\frac{\svsigma + \svmu\svsr}{\svsigma^2}}}{%
  %\Mdiag{\frac{- \svsr}{2 \svsigma^2}}}
%\svvar
%\twobyone{\Mdiag{\frac{\svsigma + \svmu\svsr}{\svsigma^2}}}{%
  %\Mdiag{\frac{- \svsr}{2 \svsigma^2}}}
%},
%\label{eqn:apx_srdist}
%\end{equation}


In practice, the population values, \pvmu, \pvmom[2], \pvvar
are all unknown, and so the asymptotic variance has to be estimated,
using the sample. 
This is impractical for large $\nstrat$, so instead one may wish to impose
some distributional assumptions on \vreti.

%Letting \svvar be some sample estimate of \pvvar,
%taken from estimating the covariance of the samples of \vreti and
%$\vreti^2$ stacked, using \eqnref{sr_deriv}
%we have the approximation
%\begin{equation}
%\svsr \approx 
%%\normlaw{\pvsnr,\oneby{\ssiz}\qoform{\svvar}{\wrapParens{\dbyd{\svsr}{\vcat{\svmu}{\svmom[2]}}}}},
%\normlaw{\pvsnr,\oneby{\ssiz}
%\onebytwo{\Mdiag{\frac{\svsigma + \svmu\svsr}{\svsigma^2}}}{%
	%\Mdiag{\frac{- \svsr}{2 \svsigma^2}}}
%\svvar
%\twobyone{\Mdiag{\frac{\svsigma + \svmu\svsr}{\svsigma^2}}}{%
	%\Mdiag{\frac{- \svsr}{2 \svsigma^2}}}
%},
%\label{eqn:apx_srdist}
%\end{equation}
%where we have plugged in sample estimates.  \cite{lo2002,mertens2002comments}

Consider the case where \vreti is drawn from a normal distribution with mean
\pvmu and covariance \pvsig.
Then, using Isserlis' Theorem \cite{Isserlis1918,HaldaneMoments},
we have
\begin{equation}
\label{eqn:elliptical_variances}
\pvvar=\twobytwo{\pvsig}{2\pvsig\Mdiag{\pvmu}}{2\Mdiag{\pvmu}\pvsig}{%
	2 \pvsig \hadm \pvsig
	+ 4 \Mdiag{\pvmu}\pvsig\Mdiag{\pvmu}
},
\end{equation}
where $\hadm$ denotes \emph{Hadamard multiplication}.

Let \RMAT be the correlation matrix of the returns, defined as
\begin{equation}
\RMAT\defeq\Mdiag{\pvsigma^{-1}}\pvsig\Mdiag{\pvsigma^{-1}},
\end{equation}
where $\pvsigma$ is the (positive) square root of the diagonal of \pvsig.
Then using the \pvvar given in \eqnref{elliptical_variances},
\eqnref{mvclt} becomes
\begin{equation}
\svsr \approx 
\normlaw{\pvsnr,\oneby{\ssiz}\wrapParens{\RMAT +
	\frac{1}{2} \Mdiag{\pvsnr} \wrapParens{\RMAT\hadm\RMAT} \Mdiag{\pvsnr}}}.
\label{eqn:apx_srdist_gaussian}
\end{equation}
(See the appendix.)
Note how in the case of scalar Gaussian returns, this reduces to the well
known standard error estimate of 
$\sqrt{\oneby{\ssiz}\wrapParens{1 + \half[\psnrsq]}}$.  
\cite{lo2002,mertens2002comments,baoestimation,pav_ssc}
In practice the correlation matrix \RMAT and the vector of \txtSNRs, \pvsnr,
have to be estimated and plugged in.

We claim that for the case of 
\emph{elliptically distributed} \vreti, 
\eqnref{apx_srdist_gaussian} can be generalized to 
\begin{equation}
\svsr \approx 
\normlaw{\pvsnr,\oneby{\ssiz}\wrapParens{\Mtx{R} +
	\frac{\kurty-1}{4} \ogram{\pvsnr} + 
	\frac{\kurty}{2} \Mdiag{\pvsnr} \wrapParens{\Mtx{R}\hadm\Mtx{R}} \Mdiag{\pvsnr}}},
\label{eqn:apx_srdist_elliptical}
\end{equation}
where \kurty is the ``kurtosis factor'', equal to one third the kurtosis of the
marginals.  \cite{vignat2007extension} 
However, elliptically distributed returns have no skew, which makes them
less than ideal for modeling returns series. 
Once again note how this equation reduces to the form of the standard error
described by Mertens in the case of $\nstrat=1$. \cite{mertens2002comments}

%Note how in the case of scalar Gaussian returns, this reduces to \eqnref{srci_lo}.
%Consider the case where \vreti is drawn from an elliptical distribution
%with mean \pvmu, covariance \pvsig, and kurtosis factor \kurty.
%When returns are Gaussian, $\kurty=1$.
%Then we have
%\begin{equation}
%\label{eqn:elliptical_variances}
%\pvvar=\twobytwo{\pvsig}{2\pvsig\Mdiag{\pmu}}{2\Mdiag{\pmu}\pvsig}{%
%\wrapParens{\kurty-1} \vdiag{\pvsig}\tr{\wrapParens{\vdiag{\pvsig}}}
	%+ 2 \kurty \pvsig \hadm \pvsig
	%+ 4 \Mdiag{\pmu}\pvsig\Mdiag{\pmu}
%}.
%\end{equation}
%\cf \exerciseref{isserlis_elliptical_drag}.

%Let \Mtx{R} be the correlation matrix of the returns, defined as
%\begin{equation}
%\Mtx{R}\defeq\Mdiag{\pvsigma^{-1}}\pvsig\Mdiag{\pvsigma^{-1}},
%\end{equation}
%where $\pvsigma$ is the (positive) square root of the diagonal of \pvsig.
%Then \eqnref{apx_srdist} becomes
%\begin{equation}
%\svsr \approx 
%\normlaw{\pvsnr,\oneby{\ssiz}\wrapParens{\Mtx{R} +
	%\frac{\kurty-1}{4} \ogram{\pvsnr} + 
	%\frac{\kurty}{2} \Mdiag{\pvsnr} \wrapParens{\Mtx{R}\hadm\Mtx{R}} \Mdiag{\pvsnr}}}.
%\label{eqn:apx_srdist_elliptical}
%\end{equation}
%\cf \exerciseref{elliptical_asymptotics}.
%Note how in the case of scalar Gaussian returns, this reduces to \eqnref{srci_lo}.

\begin{corollary}[to \theoremref{lee_etal}]
  \label{corollary:sr_est}
  Let $\vreti\sim\normlaw{\pvmu,\pvsig}$, with $\pvsnr=\pvmu \hadd \pvsigma$, where
  $\pvsigma=\vdiag{\pvsig}$.
  Let \RMAT be the correlation matrix.
  Suppose you observe \ssiz independent observations of \vreti then construct
  the \txtSR, \svsr.
  Then, conditional on $\AAA\svsr\le\bbb$, the random variable
  $$
  u=\tncdf{\trAB{\etav}{\svsr}}{\Vmin}{\Vmax}{\trAB{\etav}{\pvsnr}}{\qform{\Mtx{Q}}{\etav}}
  $$
  is Uniform on $\ccinterval{0}{1}$, where 
  \Vmin, \Vmax and 
  $\tncdf{x}{a}{b}{\pmu}{\psigsq}$
  are as in the theorem,
  and 
  $$
  \Mtx{Q} = \oneby{\ssiz}\wrapParens{\RMAT +
  \frac{1}{2} \Mdiag{\pvsnr} \wrapParens{\RMAT\hadm\RMAT} \Mdiag{\pvsnr}},
  $$
  as given in \eqnref{apx_srdist_gaussian}.

\end{corollary}

Note that the relationship between
\trAB{\etav}{\svsr} and $\Vmin$ and $\Vmax$ is such that $u$ is unlikely to be strictly monotonic increasing
with \trAB{\etav}{\svsr}, \emph{ceterus paribus}. 
However, when $\trAB{\etav}{\svsr}\to\infty$, we expect $u\to 1$, and so to test the null hypothesis
\begin{equation*}
\Hyp[0] : \trAB{\etav}{\pvsnr}=c\quad\mbox{versus}\quad \Hyp[1] :  \trAB{\etav}{\pvsnr}>c,
\end{equation*}
one should reject at the $\typeI$ level when 
$$
\tncdf{\trAB{\etav}{\svsr}}{\Vmin}{\Vmax}{c}{\qform{\Mtx{Q}}{\etav}} \ge 1 - \typeI.
$$

As stated the procedure requires that one estimate \Mtx{Q}, which requires one to estimate \RMAT and \pvsnr.
However, computing the test statistic only requires access to ${\Mtx{Q}}{\etav}$. 
In the main inferential task considered here, that vector is the 
covariance of the asset with maximum \txtSR against all the rest.
%For this case one could simply use the traditional (scalar) standard error estimators of the \txtSNR,
%for example Mertens' equation. \cite{lo2002,mertens2002comments,baoestimation,pav_ssc}

Note that \corollaryref{sr_est} has uses beyond the stated problem of performing inference on the asset with the largest \txtSR. 
For example, suppose you observe the \txtSRs of \nstrat assets, then select the asset with the largest \emph{absolute} \txtSR,
choosing whether to hold it long or short depending on the sign of the \txtSR.
You wish to perform inference on your strategy.
In this case, again reorder the assets such that the first asset has the highest absolute \txtSR, but also
flip the signs of the asset returns as necessary such that all assets have positive \txtSR. 
Then proceed as in the usual case, but add to $\AAA$ and $\bbb$ the conditional restriction that all
elements of $\svsr$ are non-negative.

One wishes to also use the result for more general problems wherein one will hold a \emph{portfolio}
of assets, conditional on some observed properties of \svsr. For example:
\begin{itemize}
  \item
    Suppose you observe the \txtSRs of \nstrat assets, then select the top $m$ by \txtSR, then
    you choose to hold an some portfolio of those $m$ assets.
    In this case set $\AAA$ and $\bbb$ to reflect the ``$m$ choose $\nstrat - m$'' relevant
    inequalities to condition on.
    %Let $\etav$ be the indicator for the top $k$ assets.
    %ACK! 2FIX: you are not concerned about the linear combination of these ...
  \item
    Suppose you observe the \txtSRs of \nstrat assets, then select all assets with \txtSR greater than
    some minimum value, \psnr[*]. Then you choose to hold some portfolio of all assets that
    pass the bar. In this case you need to modify $\AAA$ and $\bbb$ to condition on the passing
    assets having \txtSRs greater than \psnr[*] and the remaining assets having lower \txtSR.
\end{itemize}
In these cases, the test vector \etav should reflect the chosen portfolio,
but the \txtSNR of a portfolio is \emph{not} the portfolio-weighted sum (or average) of the
\txtSNRs of the constituent assets. Indeed the \txtSNR of dollar-weighted portfolio \pportw is
$\fracc{\trAB{\pvmu}{\pportw}}{\sqrt{\qform{\pvsig}{\pportw}}}.$
However, if \pportw is expressed in \emph{volatility units}, then the \txtSNR is
$\fracc{\trAB{\pvsnr}{\pportw}}{\sqrt{\qform{\RMAT}{\pportw}}}.$ Thus assuming you
can estimate volatility (and \RMAT) without error\footnote{Typically the error in 
a volatility estimate is less critical than error in the estimate of the mean.  \cite{chopra1993effect}}, 
then one could transform a dollar denominated portfolio into a volatility denominated portfolio.
From this one can perform inference using the test vector $\etav=\fracc{\pportw}{\sqrt{\qform{\RMAT}{\pportw}}}$.

One could also use the procedure to test the hypothesis that the asset with maximum 
\txtSR has higher \txtSNR than the \emph{average} \txtSNR of all assets considered. 
This is the null commonly tested by the Reality Check and its variants.
It may be of limited practical utility, however, since the selected asset may still
have inferior \txtSNR.

%UNFOLD
%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Alternative Approaches}%FOLDUP

\subsection{Bonferroni correction with simple correlation fix}

\label{subsec:fix_bonferroni}
The simple MHT approach to the problem is via a Bonferroni correction.  \cite{bretz2016multiple}
In its usual form, it assumes that the returns \vreti are independent
and normally distributed.  
In this case the marginals of \svsr are independent, and distributed
as rescaled non-central \tstat{} random variables.
So to test the null hypothesis
\begin{equation*}
\Hyp[0] : \forall_i \psnr[i]=c\quad\mbox{versus}\quad \Hyp[1] :  \exists_i \psnr[i] > c,
\end{equation*}
compute the \txtSRs, \svsr, then
reject at the $\typeI$ level when
$\sqrt{\ssiz} \max_i \svsr[i]$ exceeds $\nctqnt{1-\typeI/\nstrat}{\sqrt{\ssiz} c}{\ssiz-1}$,
the $1-\typeI/\nstrat$ quantile of the 
non-central \tstat-distribution with $\ssiz - 1$
degrees of freedom and non-centrality parameter $\sqrt{\ssiz} c$.

This simple test does does not maintain nominal type I rate in the face of
correlated assets. This can be demonstrated empirically, as we do in the sequel. 
One can get also get a theoretical hint of why this holds by considering 
the normal approximation of \svsr given in \eqnref{apx_srdist_gaussian},
then appealing to Slepian's Lemma.
Slepian's Lemma establishes that for a normally distributed Gaussian vector
with fixed mean and variance, the maximum element is `stochastically decreasing' as
correlations increase.  \cite{slepian1962one}
Intuitively the number of true independent assets is decreasing as correlation
increases.

Let us consider a simple model for the correlation matrix
\begin{equation}
\label{eqn:simple_RMAT}
\RMAT=\makerho{\rho}{\wrapParens{1-\rho}},
\end{equation}
where $\abs{\rho} \le 1$.
%and the elements of the vector $\pmone$ are each $\pm 1$.
%In the case where $\pmone=\vone$ we recapture a common positive correlation of all assets.
Now simplify \eqnref{apx_srdist_gaussian} to 
\begin{equation}
\svsr \approx \normlaw{\pvsnr,\oneby{\ssiz}\RMAT},
  \label{eqn:apx_srdist_simple}
\end{equation}
which is reasonable in the case of the small \txtSNRs likely to be encountered in practice.
Then under the null hypothesis that $\pvsnr=\pvsnr[0]$, one observes
\begin{equation}
  \vect{z} = \sqrt{\ssiz}\ichol{\RMAT} \wrapParens{\svsr - \pvsnr[0]} \approx \normlaw{\vzero,\eye},
  \label{eqn:bonf_zform_simple}
\end{equation}
where $\ichol{\RMAT}$ is the inverse of the (symmetric) square root of $\RMAT$.

Under the assumed form for \RMAT given in \eqnref{simple_RMAT}, it is simple to confirm that
\begin{equation}
  \label{eqn:simple_RMAT_ichol}
  \ichol{\RMAT} = \makerho{\oneby{\nstrat}\wrapParens{\oneby{\sqrt{1-\rho+\nstrat\rho}} - \oneby{\sqrt{1-\rho}}}}{\wrapParens{1-\rho}^{-1/2}}.
\end{equation}
(This relation holds if we replace \ogram{\vone} in \RMAT with \ogram{\pmone} where
$\pmone$ is any vector whose elements are $\pm 1$. However in this case
we will lose the order-preserving property.)

Now it is simple to confirm that in this case the
transform induced by $\ichol{\RMAT}$ is ``order-preserving.''
That is, if $\vect{a} = \ichol{\RMAT}\vect{b}$ and $b_i \le b_j$ then $a_i \le a_j$.
As a consequence, if $i$ is the maximal element of $\wrapParens{\svsr - \pvsnr[0]}$,
then $i$ is the maximal element of $\vect{z}$.  Let us assume, again, that by
convention we have reordered the elements such that the first element of
\svsr is the maximum. Then to test the null hypothesis
$\forall_i \psnr[i]=c$, compute
\begin{equation}
  z_1 = \frac{\sqrt{\ssiz}\ssr[1]}{\sqrt{1-\rho}} + 
  \wrapParens{\oneby{\sqrt{1-\rho+\nstrat\rho}} - \oneby{\sqrt{1-\rho}}} \sqrt{\ssiz}\wrapParens{\frac{\tr{\vone}\svsr}{\nstrat} - c},
  \label{eqn:fix_bonf_z}
\end{equation}
and reject the null hypothesis when $z_1$ is bigger than 
$\qnorm{1-\typeI/\nstrat},$ the $1-\typeI/\nstrat$ quantile of the normal distribution.
In practice $\rho$ must be estimated. This could be done by computing the correlations
of the first asset against all others, then taking the average.

Note that the test statistic $z_1$ in \eqnref{fix_bonf_z} depends on elements of
\svsr other than \ssr[1]. 
Indeed it depends on the average value among the \ssr[i].
This may not be desireable, as it would reject as one of the \ssr[i] went to $-\infty$ for $i\ne 1$.
Moreover, the statistic $z_1$ does not seem to be entirely ``about'' \ssr[1], but
is computed from all elements of \svsr.
To rectify this, one is tempted to rotate the $\vect{z}$ from \eqnref{bonf_zform_simple}
to be maximally aligned with $\basev[1]$. 
This is an area of continued research.
%\footnote{Version 3 of this paper erroneously
%claimed to have found such a rotation, yielding a simple correction factor. 
%We regret the error, though we still believe such a rotation is possible.}.

%To rectify this, note that the representation in \eqnref{bonf_zform_simple} is not unique,
%rather it can be rotated by any orthonormal matrix \Mtx{Q}:
%\begin{equation*}
  %\vect{z} = \sqrt{\ssiz}\Mtx{Q}\ichol{\RMAT} \wrapParens{\svsr - \pvsnr[0]} \approx \normlaw{\vzero,\eye}.
%\end{equation*}
%If we choose \Mtx{Q} such that $\Mtx{Q}\ichol{\RMAT}\basev[1] = c \basev[1]$, then $z_1$ will depend
%only on \ssr[1], the maximal \txtSR. 
%Note that by construction of \Mtx{Q} we have
%$$
%c^2 = \qform{\ichol{\RMAT}\gram{\Mtx{Q}}\ichol{\RMAT}}{\basev[1]} = 
 %\qiform{\RMAT}{\basev[1]}.
%$$
%Then we can rewrite \eqnref{fix_bonf_z} as
%\begin{equation}
  %z_1 = \sqrt{\ssiz}\sqrt{\qiform{\RMAT}{\basev[1]}} \wrapParens{\ssr[1] - \psnr[0]} \approx \normlaw{0,1}.
%\end{equation}
%Again, reject the null hypothesis when $z_1$ is bigger than 
%$\qnorm{1-\typeI/\nstrat},$ the $1-\typeI/\nstrat$ quantile of the normal distribution.

%Under the assumed form of \RMAT, we have
%\begin{equation*}
  %\minv{\RMAT} = \makerho{\frac{-\rho}{\wrapParens{1-\rho}\wrapBracks{\nstrat \rho + 1 - \rho}}}{\frac{1}{1-\rho}},
%\end{equation*}
%and thus 
%%c = \sqrt{\frac{\nstrat\rho + 1 - 2 \rho}{1-\rho}}.
%\begin{equation*}
  %z_1 = \sqrt{\ssiz}
  %\sqrt{\frac{\nstrat\rho + 1 - 2 \rho}{1-\rho}}
  %\wrapParens{\ssr[1] - \psnr[0]}.
%\end{equation*}

Note that we did not have to make the simplifying assumption that led from 
\eqnref{apx_srdist_elliptical} to \eqnref{apx_srdist_simple} given the
form we assumed for \RMAT. That is, assuming $\RMAT=\makerho{a_2}{a_1}$,
then under the null hypothesis that $\pvsnr=\psnr[0]\vone$, 
\eqnref{apx_srdist_elliptical} becomes
\begin{equation}
\svsr \approx 
  \normlaw{\pvsnr,\oneby{\ssiz}\wrapParens{\makerho{a_2'}{a_1'}}},
\end{equation}
for some constants $a_1', a_2'$ which depend on $a_1, a_2, \kurty$ and $\psnr[0]$.
One could then proceed as above, constructing a $z_1$ statistic.


<<'scratch_foo', include=FALSE, eval=FALSE, warning=FALSE, message=FALSE, cache=TRUE>>=
p <- 12
rho <- 0.3
set.seed(1234)
w <- sign(rnorm(p))
R <- (1-rho) * diag(p) + rho * w %*% t(w)
ihR <- (1/sqrt(1-rho)) * diag(p) + (1/p) * ((1/sqrt(1-rho+p*rho)) - (1/sqrt(1-rho))) * (w%*%t(w))
hR <- solve(ihR)
R - hR %*% hR
max(abs( R - hR %*% hR  ))
@

% this is not true? I was lead astray by Lopes;
% Slepian lemma only gives us a best case, and is useless here. 
% rats.
%
\paragraph{Bonferroni correction for arbitrary correlation structure}

The Bonferroni correction outlined above is strictly only applicable
to the rank-one correlation matrix, 
$\RMAT=\makerho{\rho}{\wrapParens{1-\rho}}$.
To apply the correction to any correlation matrix with positive entries,
Slepian's lemma allows us to appeal to a worst-case rank-one
correlation matrix.
\cite{slepian1962one,zeitouni2015gaussian}
Let $\xxx \sim \normlaw{\vzero,\RMAT}$ and
$\yyy \sim \normlaw{\vzero,\makerho{\rho}{\wrapParens{1-\rho}}}$,
for $\rho \ge 0$ where $\RMAT[i,j] \ge \rho$
for all $i\ne j$.
By Slepian's lemma, $\Pr{\max_i \xxs[i] \ge t} \le \Pr{\max_i \yys[i] \ge t}.$


Then assume the correlation of returns is \RMAT, 
where $\RMAT[i,j] \ge \rho$ for $i\ne j$ for some $\rho \ge 0$;
to test the null hypothesis
$\forall_i \psnr[i]=c$, compute $z_1$ as in \eqnref{fix_bonf_z},
plugging in $\rho$, 
and reject the null hypothesis when $z_1$ is bigger than 
$\qnorm{1-\typeI/\nstrat},$ the $1-\typeI/\nstrat$ quantile of the normal distribution.
This procedure has (approximate) type I rate \emph{no greater} than $\typeI$.

\subsection{Testing against one-sided alternatives}

Another obvious approach to the problem is to appeal to the
normal approximation of 
\eqnref{apx_srdist_gaussian} or \eqnref{apx_srdist_elliptical}, then
use well known techniques in testing of a multivariate normal
against a one-sided alternative.  \cite{nla.cat-vn3800977,vock2007}
That is, the usual multivariate procedure to test the null hypothesis 
$\Hyp[0] : \forall_i \psnr[i]=c$ under a normal approximation
would be via Hotelling's $T^2$ test.  \cite{anderson2003introduction,press2012applied}
However we are not interested in the case where some of
the $\psnr[i]$ are less than $c$.
%One-sided multivariate tests were designed for this task.
%They may not scale well to the case of large \nstrat, except perhaps
%under simple models for correlation.
%However, they often require that one construct a statistic like Hotelling's $T^2$,
%which in our case would involve inverting the covariance of $\svsr$.
%This will not scale well computationally to allow testing with large \nstrat.

One-sided tests will not scale well to the case of large \nstrat, except perhaps
under simple models for correlation.
Consider testing the following null
\begin{equation*}
\Hyp[0] : \forall_i \psnr[i]=\psnr[0]\quad\mbox{versus}\quad \Hyp[1] :  
\forall_i \psnr[i]\ge\psnr[0]\,\,\mbox{and}\,\, \exists_j \psnr[j] > \psnr[0],
\end{equation*}
subject to the rank one correlation structure of \eqnref{simple_RMAT} with $\rho \ge 0$.
Again assume the approximation of \eqnref{apx_srdist_simple},
\begin{equation*}
\svsr \approx \normlaw{\pvsnr,\oneby{\ssiz}\RMAT}.
\end{equation*}
Letting $\stvsr = \ichol{\RMAT}\svsr$ and 
$\ptvsnr = \ichol{\RMAT}\pvsnr$, the normal approximation can be rewritten as
\begin{equation*}
  \sqrt{\ssiz}\stvsr \approx \normlaw{\sqrt{\ssiz}\ptvsnr,\eye}.
\end{equation*}
The inverse square root of \RMAT, given in \eqnref{simple_RMAT_ichol}, is order-preserving.
Moreover, 
\begin{equation*}
  \ichol{\RMAT} \vone = c\vone,
\end{equation*}
for $c=\wrapParens{1+\wrapParens{\nstrat-1}\rho}^{-\halff} > 0$.
From the order-preserving nature of \ichol{\RMAT}, the null and alternative hypotheses can be expressed
as 
\begin{equation*}
\Hyp[0] : \forall_i \ptsnr[i]=c \psnr[0]\quad\mbox{versus}\quad \Hyp[1] :  
\forall_i \ptsnr[i]\ge c \psnr[0]\,\,\mbox{and}\,\, \exists_j \ptsnr[j] > c \psnr[0].
\end{equation*}

We can then appeal to the simple chi-bar square test.  \cite{nla.cat-vn3800977}
First transform the vector of \txtSRs to \stvsr via 
$$
\stvsr=\ichol{\RMAT}\svsr= c\ssravg\vone + \wrapParens{1-\rho}^{-\halff}\wrapParens{\svsr - \ssravg \vone},
$$
where $\ssravg$ is the average of the sample \txtSRs.
Then compute
\begin{equation}
  \label{eqn:chibstat}
  \bar{x}^2 = \ssiz \sum_i \pospsq{\wrapParens{\stsr[i] - c \psnr[0]}},
\end{equation}
where $\posp{y}$ is the positive part of $y$, \ie $\posp{y}=y$ if $y > 0$ and zero otherwise.
Then compute the CDF of the corresponding chi-bar square distribution as
\begin{equation}
  \label{eqn:chibp}
  Q = \sum_{i=0}^{\nstrat} w_i {\chisqcdf{\bar{x}^2}{i}},
\end{equation}
where $\chisqcdf{i}{x}$ is the cumulative distribution of the $\chi^2$ distribution
with $i$ degrees of freedom, and $w_i$ are the chi-bar square weights.
In this case they are defined as
$$
w_i = {\nstrat \choose i}{2^{-\nstrat}}.
$$
Reject the null hypothesis at the \typeI level if $1-Q \le \typeI$.

Note that, as with the Bonferroni correction, the test statistic $\bar{x}^2$ is
computed on all elements of \svsr, and thus the decision to reject the null
may not be ``about'' \ssr[1].
In testing we will see that the one-sided test is highly susceptible to distribution of the \pvsnr,
moreso than the Bonferroni correction.

We note that under this setup it is also easy to use Follman's test, which is a very simple procedure with
increased power against one-sided alternatives. \cite{10.2307/2291680}
Here we would compute
$$
g^2 = \ssiz^2 c^2 \wrapParens{\ssravg - \psnr[0]}^2 + \frac{\ssiz}{1-\rho}\sum_i \wrapParens{\ssr[i]-\ssravg}^2,
$$
and reject at the \typeI level if both $1-\chisqcdf{g^2}{\nstrat} \le 2 \typeI$ and $\ssravg > \psnr[0]$.



\subsection{Subspace approximation}%FOLDUP

%Lastly we mention that when asset returns are well approximated
%by a linear combination of $m$ `latent' returns, we can
%view the asset with maximum \txtSR as holding the sample \txtMP
%over the latent returns, then appeal to the theory of the
%statistic of the \txtMP via Hotelling's $T^2$ statistic.  \cite{pav_maxsharpe_two}
%This approach requires further development.

Another potential approach to the problem which may be useful in
the case where returns are highly correlated, as one expects when
returns are from backtested quantitative trading strategies, is via 
a subspace approximation.
%a Markowitz approximation. 
First we assume that the $\bby{\ssiz}{\nstrat}$ matrix of returns, \mreti
can be approximated by a $m$-dimensional subspace
$$
\mreti \approx \Mtx{Y}\Mtx{W},
$$
where $\Mtx{Y}$ is a $\bby{\ssiz}{m}$ matrix of `latent' returns,
and $\Mtx{W}$ is a $\bby{m}{\nstrat}$ `loading' matrix.

Now the column of \mreti with maximal \svsr has \txtSR that
is smaller than
$$
\ssropt \defeq \max_{\vect{\nu}} \frac{\trAB{\svmu}{\vect{\nu}}}{\sqrt{\qform{\svsig}{\vect{\nu}}}},
$$
where $\svmu$ is the $m$-vector of the (sample) means of columns of \Mtx{Y} and 
$\svsig$ is the sample covariance matrix.
This maximum takes value
$$
\ssropt = \sqrt{\qiform{\svsig}{\svmu}},
$$
which is, up to scaling, Hotelling's $T^2$ statistic.

Under the null hypotheis that the rows of \Mtx{Y} are independent draws from a Gaussian
random variable with zero mean, then
$$
\frac{\wrapParens{\ssiz-m}\ssrsqopt}{m \wrapParens{\ssiz-1}}
$$
follows an $F$ distribution with $m$ and $\ssiz-m$ degrees of 
freedom.  
Under the alternative it follows a non-central $F$ distribution.
\cite{anderson2003introduction,press2012applied}
Via this upper bound $\ssr[1] \le \ssropt$, one can then perform
tests on the null hypothesis $\forall_i \psnr[i]=0$.

However, this approach requires that one estimate $m$, the
dimensionality of the latent subspace. Moreover, the subspace
approximation may not be very good. It would seem that to
get near equality of \ssr[1] and \ssropt, the columns
of \mreti would have to contain both positive and negative
exposure to the columns of \Mtx{Y}. This in turn should result in 
mixed correlation of asset returns, which we may not observe
in practice. 
Finally, empirical testing indicates this approach requires further 
development.  \cite{pav_maxsharpe_two}

%\cite{kubokawa1993estimation}
%UNFOLD
%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical Results}%FOLDUP

\subsection{Simulations under the null}

% motherload first sims%FOLDUP
<<'motherload_sim_funcs',echo=FALSE,eval=TRUE,cache=TRUE>>=
suppressMessages({
	library(dplyr)
  library(doFuture)
  library(epsiwal)
})

tgen <- function(n,mu,sigma,df) {
  rescal <- sqrt((df-2)/df)
  rescal * mvtnorm::rmvt(n,df=df,delta=mu/rescal,sigma=sigma,type='shifted')
}

dosim <- function(nsim,nday,R,mu,testmu=mu,ret_dist=c('gaussian','t4','t5','t6','t20'),
                  estimate_parms=FALSE,higher_order=FALSE) { 
	ret_dist <- match.arg(ret_dist)
  p <- length(mu)
  zeta <- mu / diag(R)
  if (!estimate_parms) {
    if (higher_order) {
      kapp <- switch(ret_dist,
                     gaussian=1,
                     t4=Inf,
                     t5=3,
                     t6=2,
                     t20=18/16)
      mySigma <- (1/nday) * (R + ((kapp-1)/4) * outer(zeta,zeta,'*') + (kapp/2) * diag(zeta) %*% (R * R) %*% diag(zeta))
    } else {
      mySigma <- (1/nday) * (R + (1/2) * diag(zeta) %*% (R * R) %*% diag(zeta))
    }
  }

  testzeta <- testmu / diag(R)
  generator <- switch(ret_dist,
                      gaussian=function(n) mvtnorm::rmvnorm(n,mean=mu,sigma=R),
                      t4=function(n) tgen(n,mu=mu,sigma=R,df=4),
                      t5=function(n) tgen(n,mu=mu,sigma=R,df=5),
                      t6=function(n) tgen(n,mu=mu,sigma=R,df=6),
                      t20=function(n) tgen(n,mu=mu,sigma=R,df=20))

  A1 <- cbind(-1,diag(p-1))
  these_sims <- replicate(nsim,{
    X <- generator(nday)
    srs <- colMeans(X) / apply(X,2,sd)

    if (estimate_parms) {
      Rhat <- cov2cor(cov(X))
      if (higher_order) {
        kapp <- median(apply(X,2,moments::kurtosis)) / 3
        mySigma <- (1/nday) * (Rhat + ((kapp-1)/4) * outer(srs,srs,'*') + (kapp/2) * diag(srs) %*% (Rhat * Rhat) %*% diag(srs))
      } else {
        mySigma <- (1/nday) * (Rhat + (1/2) * diag(srs) %*% (Rhat * Rhat) %*% diag(srs))
      }
    }

    y <- srs

    # collect the maximum, so reorder the A above
    yord <- order(y,decreasing=TRUE)
    revo <- seq_len(p)
    revo[yord] <- revo

    A <- A1[,revo]
    nu <- rep(0,p)
    nu[yord[1]] <- 1
    b <- rep(0,p-1)

    foo <- epsiwal::pconnorm(y=y,A=A,b=b,eta=nu,mu=testzeta,Sigma=mySigma)
    c(max(srs),foo,zeta[yord[1]])
  })
  data_frame(simvals=as.numeric(these_sims[1,]),
             pvals=as.numeric(these_sims[2,]),
             popvals=as.numeric(these_sims[3,]))
}
parsim <- function(nsim,nday,R,mu,testmu=mu,estimate_parms=FALSE,ret_dist='gaussian',higher_order=FALSE,nnodes=7) {
  if (nsim > 2*nnodes) {
    # do in parallel.
    nper <- table(1 + ((0:(nsim-1) %% nnodes))) 
    retv <- foreach(i=1:nnodes,.export = c('nday','R','mu','testmu','estimate_parms','ret_dist','higher_order',
                                           'dosim')) %dopar% {
      dosim(nsim=nper[i],nday=nday,R=R,mu=mu,testmu=testmu,ret_dist=ret_dist,
            estimate_parms=estimate_parms,higher_order=higher_order)
    } %>%
      bind_rows()
  } else {
    retv <- dosim(nsim=nsim,nday=nday,R=R,mu=mu,testmu=testmu,ret_dist=ret_dist,
            estimate_parms=estimate_parms,higher_order=higher_order)
  }
  retv
}
ks_sim <- function(rho,nday,nlatf,max_zeta,estimate_parms=FALSE,ret_dist='gaussian',higher_order=FALSE,nsim=100) {
  R <- pmin(diag(nlatf) + rho,1)  
  mu <- seq(0,max_zeta,length.out=nrow(R))
  sim <- parsim(nsim,nday=nday,R=R,mu=mu,ret_dist=ret_dist,
                estimate_parms=estimate_parms,higher_order=higher_order)
  broom::tidy(ks.test(sim$pvals,punif))
}
#sim <- parsim(nsim,nday=nday,R=R,mu=mu,estimate_parms=FALSE,ret_dist='t5')
#plot(ecdf(sim$pvals))
@

\subsubsection{Gaussian returns, infeasible estimator}

<<'motherload_sims_run',echo=FALSE,eval=TRUE,cache=TRUE,dependson=c('motherload_sim_funcs','runtime')>>=
suppressMessages({
  library(doFuture)
})
nsim <- ceiling(1e5/max(1,RUNTIME_PARAM))
ope <- 252
nyr <- 5
nday <- round(nyr*ope)
rho <- 0.7
nlatf <- 100
zetrange <- c(-0.1,0.1)

R <- pmin(diag(nlatf) + rho,1)  
mu <- seq(min(zetrange),max(zetrange),length.out=nrow(R))

registerDoFuture()
plan(multiprocess)
set.seed(4321)
sim <- parsim(nsim,nday=nday,R=R,mu=mu,estimate_parms=FALSE)
plan(sequential)
@


First we seek to establish if, and under what circumstances, 
the normal approximation of \eqnref{apx_srdist_gaussian} is sufficiently accurate to 
give nominal coverage under the conditional estimation procedure.
First we test a single case of $\nstrat=\Sexpr{nlatf}$ using a correlation
matrix that is $\rho=\Sexpr{rho}$ on the off-diagonals:
$\RMAT=\makerho{\Sexpr{rho}}{\Sexpr{1-rho}}$.
We generate Gaussian returns over $\Sexpr{nday}$ days, approximately
$\Sexpr{nyr}$ years worth for equity returns.
We let $\pvsnr$ range uniformly from $\Sexpr{min(zetrange)}\dayto{-\halff}$ to
$\Sexpr{max(zetrange)}\dayto{-\halff}$. We compute the \txtSRs of
each asset's simulated returns, find the asset with maximum \txtSR,
then compute a p-value using \theoremref{lee_etal}. 
Since we wish to assess the accuracy of the normal approximation,
we use the actual population value of \RMAT, and the \pvsnr to compute
the covariance via \eqnref{apx_srdist_gaussian}.
This is not, of course, how the test would be applied in practice since
\RMAT and \pvsnr have to be estimated.
Moreover, we with to check coverage of the procedure under the null, so 
we use the actual $\trAB{\etav}{\pvsnr}$ in our test.

We repeat this experiment \Sexpr{nsim} times and collect the resultant putative p-values.
We would like to Q-Q or P-P plot these p-values, 
as evidence that they are near uniform, 
but the large sample size presents some challenges.
Instead, we choose some selected small $q$ (like 0.05 or 0.01), and
compute the proportion of our p-values $\le q$. We then subtract $q$.
This value, call it $\Delta$, should be near zero. We plot $\Delta$ against
$q$, with errorbars around the $x$ axis reflecting the area where we would
expect the points to fall roughly 95\% of the time. 
Those errorbars are computed via the Binomial law, but do not always have width
of exactly 95\% because of the finite sample size.
However, the plot suggests that the p-values are indeed uniformly distributed,
and that the procedure has near nominal type I rate when selecting a cutoff
near the selected $q$.

<<'motherload_sims_hidden_plotz',dependson=c('motherload_sims_run','plot_funcs'),echo=FALSE,eval=TRUE,cache=TRUE,fig.width=10,fig.height=5.5,out.width="0.975\\textwidth",fig.pos='h',fig.cap=fig_cap>>=
alpha <- 0.05
fig_cap <- paste0('The computed p-values from the conditional estimation procedure over ',nsim,
                  ' simulations are compared to a uniform law. ',
                  'Given the large sample size, a Q-Q plot is visually hard to interpret. ',
                  'Instead, for selected $q$, we compute the proportion of our putative p-values less than $q$. ',
                  'We then compute $\\Delta = \\wrapParens{\\mbox{Prop }\\le q} - q$ against $q$. ',
                  'Using the binomial law, we plot approximate error bars around the $x$ axis that indicate where the points should fall with approximately $',100*(1-alpha),'\\%$ confidence. ',
                  'Simulations use the exact \\RMAT and \\pvsnr to compute the covariance matrix. ',
                  'The plot supports uniformity of the putative p-values. ')

suppressMessages({
	library(dplyr)
	library(ggplot2)
})
ph <- cdf_helper(sim$pvals,qts=10^seq(-3,-1,length.out=9),alpha=alpha) %>%
  mutate(Delta=emp_cdf-qts,ymin=ci_lo-qts,ymax=ci_hi-qts) %>%
  ggplot(aes(qts,Delta,ymin=ymin,ymax=ymax)) +
  geom_point() + 
  geom_errorbar(alpha=0.5,width=0.03) + 
  geom_abline(slope=0,intercept=0,linetype=3,alpha=0.7) + 
  #scale_x_sqrt() + scale_y_sqrt() + 
  scale_x_log10() + 
  labs(x='quantile q',
       y='Delta : (proportion of observed p-values <= q) - q',
       title='Sampled empirical CDF, p-values from conditional estimation procedure')
print(ph)
@
%UNFOLD

\clearpage

\subsubsection{Gaussian returns, feasible estimator}
% causal sims%FOLDUP

<<'causal_sims',echo=FALSE,eval=TRUE,cache=TRUE,dependson=c('motherload_sim_funcs')>>=
# 160fee33-ad1d-4dc1-875e-04ff4cb8042c 
registerDoFuture()
plan(multiprocess)
set.seed(4321)
causim <- parsim(nsim,nday=nday,R=R,mu=mu,estimate_parms=TRUE)
plan(sequential)
@

While these experiments suggest the normal approximation leads to nearly uniform p-values
under the null, they use the (unknown) population values of \RMAT and \pvsnr to compute
the variance-covariance of \svsr.
So we repeat the experiments, but plug in the usual sample estimate of covariance and the
vector of \txtSRs into \eqnref{apx_srdist_gaussian} to estimate the covariance
matrix of \svsr. 
Other than this change, we repeat the previous experiment, 
performing $\Sexpr{nsim}$ simulations, 
setting $\nstrat=\Sexpr{nlatf}$,
$\RMAT=\makerho{\Sexpr{rho}}{\Sexpr{1-rho}}$.
$\ssiz=\Sexpr{nday}\dayto{}$, \etc
In \figref{causal_sims_log_plotz} we present a sampled CDF plot of the log p values, as above.
Again the simulations are consistent with the procedure having nominal coverage.
This is not surprising, because, as noted above, the statistical test
only requires us to estimate the standard error of the \txtSR of the
asset with maximum \txtSR, and so does not greatly rely on the $\nstrat^2$
elements of the estimate of \RMAT.

<<'causal_sims_log_plotz',dependson=c('causal_sims','plot_funcs'),echo=FALSE,eval=TRUE,cache=TRUE,fig.width=10,fig.height=5.5,out.width="0.975\\textwidth",fig.pos='h',fig.cap=fig_cap>>=
alpha <- 0.05
fig_cap <- paste0('The computed p-values from the conditional estimation procedure using sample estimates of the covariance matrix, \\RMAT, ',
                  'and the vector of \\txtSNRs, \\pvsnr are used in this sampled CDF plot. ',
                  'P values are computed over ',nsim,' simulations. ',
                  'For selected $q$, we compute the proportion of our putative p-values less than $q$. ',
                  'We then compute $\\Delta = \\wrapParens{\\mbox{Prop }\\le q} - q$ against $q$. ',
                  'Using the binomial law, we plot approximate error bars around the $x$ axis that indicate where the points should fall with approximately $',100*(1-alpha),'\\%$ confidence. ',
                  'Simulations use the exact \\RMAT and \\pvsnr to compute the covariance matrix. ',
                  'The plot supports uniformity of the putative p-values. ')

suppressMessages({
	library(dplyr)
	library(ggplot2)
})

ph <- cdf_helper(causim$pvals,qts=10^seq(-3,-1,length.out=9),alpha=alpha) %>%
  mutate(Delta=emp_cdf-qts,ymin=ci_lo-qts,ymax=ci_hi-qts) %>%
  ggplot(aes(qts,Delta,ymin=ymin,ymax=ymax)) +
  geom_point() + 
  geom_errorbar(alpha=0.5,width=0.03) + 
  geom_abline(slope=0,intercept=0,linetype=3,alpha=0.7) + 
  scale_x_log10() + 
  labs(x='quantile q',y='Delta : (proportion of observed p-values <= q) - q',
       title='Sampled empirical CDF, p-values from conditional estimation procedure, feasible estimates')
print(ph)
@
%UNFOLD

\clearpage

\subsubsection{Gaussian returns, feasible estimator, sensitivity}

% many sims with K-S stat?%FOLDUP

<<'many_sims',echo=FALSE,eval=TRUE,cache=TRUE,dependson=c('motherload_sim_funcs','runtime')>>=
# 5b3a9d41-2751-45b6-b6af-a4e43db1f298 

suppressMessages({
  library(tidyr)
  library(tibble)
})
params <- tidyr::crossing(tibble::tribble(~rho,0,0.4,0.8),
                          tibble::tribble(~nday,round(0.5*ope),ope,2*ope,4*ope,8*ope),
                          tibble::tribble(~nlatf,50,100,200),
                          tibble::tribble(~max_zeta,0.1))

ks_nsim <- ceiling(5e4/max(1,RUNTIME_PARAM))

registerDoFuture()
plan(multiprocess)
set.seed(1234)
#system.time({
resu <- params %>%
  group_by(rho,nday,nlatf,max_zeta) %>%
    summarize(resu=list(ks_sim(rho=rho,nday=nday,nlatf=nlatf,max_zeta=max_zeta,estimate_parms=TRUE,nsim=ks_nsim))) %>%
  ungroup() %>%
  unnest()
#})
plan(sequential)
@


This kind of ``proof by eyeball'' is somewhat unsatisfying, and does not scale up to the task
of finding where the approximation is accurate.
To measure the uniformity of our p-values, we generate some via simulations as described above,
then compute the Kolmogorov-Smirnov statistic against a uniform distribution. 
\cite{Marsaglia:Tsang:Wang:2003:JSSOBK:v08i18}
You can think of the K-S statistic as the maximum absolute deviance of a point away
from the $y=x$ line in a P-P plot like 
\figref{motherload_sims_hidden_plotz}.

So we repeat the previous experiments, using a feasible estimator of the covariance matrix of \svsr. 
Again we draw returns from a Gaussian distribution.
We let $\ssiz$ vary from $\Sexpr{min(params$nday)}$ to $\Sexpr{max(params$nday)}$;
we let $\nstrat$ vary from $\Sexpr{min(params$nlatf)}$ to $\Sexpr{max(params$nlatf)}$;
we let $\rho$ vary from $\Sexpr{min(params$rho)}$ to $\Sexpr{max(params$rho)}$
where we take 
$\RMAT=\makerho{\rho}{\wrapParens{1-\rho}}$;
we take $\pvsnr$ to be a uniform sequence from $0$ to 
$\Sexpr{unique(params$max_zeta)}\dayto{-\halff}$.
For each setting of the parameters in the Cartesian product we perform
$\Sexpr{ks_nsim}$ simulations, computing p values from the feasible estimator.

In \figref{many_sims_plotz} we plot those K-S statistics against $\ssiz$,
with facets for $\rho$ and $\nstrat$. 
All else equal, we expect the approximation to be worse, and thus the K-S statistics
to be higher, for smaller \ssiz and larger \nstrat.
%For the most part this does not seem born out, and there does not seem to be
%a general pattern to the quality of our approximation.
This pattern is somewhat visible in the plots, 
although large $\rho$ seems to reduce the number of `pseudo-assets' in that
relationship.
However, with the given limited evidence,
we cannot claim to have definitively established where our procedure breaks down,
but warn users that the $\nstrat \gg \ssiz$ cases are likely to be problematic
in the sense that nominal type I rates may not be maintained.

<<'many_sims_plotz',dependson=c('many_sims'),echo=FALSE,eval=TRUE,cache=TRUE,fig.width=10,fig.height=5.5,out.width="0.975\\textwidth",fig.pos='h',fig.cap=fig_cap>>=
fig_cap <- paste0('Kolmogorov-Smirnov statistics summarizing uniformity of the test statistic $u$ are plotted versus $\\ssiz$ with facets for $\\nstrat$ and $\\rho$.  ',
                  'Broadly we see that the test statistic is less uniform in the regime where $\\nstrat\\gg\\ssiz$, but that a large positive $\\rho$ perhaps reduces the number of assets in that relationship.')
# 460af323-8484-4187-8af1-68b25eff11cb 
resu %>%
  mutate(raty=nday/nlatf) %>%
  rename(p=nlatf) %>%
  ggplot(aes(nday,statistic)) + 
  geom_point() + geom_line() + 
  scale_x_log10() + 
  facet_grid(p~rho,labeller=label_both) +
  labs(x='n (days)',
       y='K-S statistic',
       title='K-S statistic for simulated p values')
@
%UNFOLD

\subsubsection{$\tstat{}$ returns, feasible estimator}
%FOLDUP
<<'causal_tfive_sims',echo=FALSE,eval=TRUE,cache=TRUE,dependson=c('motherload_sim_funcs','runtime')>>=
# 981f1d81-9dba-42e4-82c5-eac0baf80699 
nsim <- ceiling(1e4/max(1,RUNTIME_PARAM))
registerDoFuture()
plan(multiprocess)
set.seed(4321)
ok_tsim <- parsim(nsim,nday=nday,R=R,mu=mu,estimate_parms=FALSE,ret_dist='t5',higher_order=TRUE)
set.seed(4321)
badtsim <- parsim(nsim,nday=nday,R=R,mu=mu,estimate_parms=FALSE,ret_dist='t5',higher_order=FALSE)
plan(sequential)
@

The simulations above were carried out assuming Gaussian returns, and using
\eqnref{apx_srdist_gaussian} to compute the covariance matrix of \svsr.
Gaussian returns are not a good model for real asset returns,
so we repeat those simulations with returns drawn from a multivariate $\tstat{}$-distribution
with $5$ degrees of freedom.  \cite{Lin1972339,kotz2004multivariate}
Again we perform $\Sexpr{nsim}$ simulations with
$\nstrat=\Sexpr{nlatf}$,
$\RMAT=\makerho{\Sexpr{rho}}{\Sexpr{1-rho}}$,
$\ssiz=\Sexpr{nday}\dayto{}$, \etc
We perform inference twice, once using 
\eqnref{apx_srdist_gaussian},
and once using \eqnref{apx_srdist_elliptical} where we have estimated
the kurtosis factor, \kurty, by taking the median of the sample marginal 
kurtosises of the assets. 
In \figref{causal_tfive_sims_plotz} we present the subsampled empirical CDF plots
on the log-transformed p-values under the two methods of estimating
the covariance of \svsr.
There is little difference in the performance of the two sets of simulations,
though without the correction, the procedure is slightly anti-conservative, while with the correction
it is slightly conservative.
We remain cautiously optimistic that for large \ssiz, one need not correct
\eqnref{apx_srdist_gaussian} to account for non-normal returns.


<<'causal_tfive_sims_plotz',dependson=c('causal_tfive_sims'),echo=FALSE,eval=TRUE,cache=TRUE,fig.width=10,fig.height=5.5,out.width="0.975\\textwidth",fig.pos='h',fig.cap=fig_cap>>=
alpha <- 0.05
fig_cap <- paste0('The computed p-values from the conditional estimation procedure over ',nsim,
                  ' simulations are plotted using our sampled CDF procedure. ',
                  'Returns are drawn from a $\\tstat{}\\wrapParens{5}$ distribution.  ',
                  'Sample estimates are used to construct the variance-covariance matrix of \\svsr.  ',
                  'The experiments are performed twice: first assuming that returns are Gaussian; ',
                  'then assuming returns are elliptical with unknown kurtosis factor that we estimate from the sample.  ',
                  'Both procedures give near-uniform p values. ',
                  'Without correcting for higher order moments, the procedure has higher than nominal type I rate; ',
                  'with the correction it has slightly smaller than the nominal rate. ')

suppressMessages({
	library(dplyr)
	library(ggplot2)
})
ph <- rbind(ok_tsim %>% mutate(higher_order=TRUE),
            badtsim %>% mutate(higher_order=FALSE)) %>%
  group_by(higher_order) %>%
    summarize(fooret=list(cdf_helper(pvals,qts=10^seq(-3,-1,length.out=9),alpha=alpha))) %>%
  ungroup() %>%
  tidyr::unnest() %>%
  mutate(Delta=emp_cdf-qts,ymin=ci_lo-qts,ymax=ci_hi-qts) %>%
  ggplot(aes(qts,Delta,ymin=ymin,ymax=ymax,color=higher_order)) +
  geom_point() + 
  geom_errorbar(alpha=0.33,width=0.03,color='black') + 
  geom_abline(slope=0,intercept=0,linetype=3,alpha=0.7) + 
  scale_x_log10() + 
  labs(x='quantile q',y='Delta : (proportion of observed p-values <= q) - q',
       color='higher order?',
       title='Sampled empirical CDF, p-values from conditional estimation procedure, feasible estimates')
print(ph)
@
%UNFOLD

\clearpage

\subsection{Simulations under the alternative}

We wish to test the power of the method under the alternative hypothesis.
However, it is hard to state exactly what constitutes the alternative.
One interpretation is that we condition on $\psnr[1] > 0$, where again
the indexing is such that $\ssr[1]$ was the maximum over $\nstrat$ assets;
then we estimate the probability of (correctly) rejecting $\psnr[1] = 0$
versus $\psnr[1]$. However, we suspect that the power, as described in 
this way, would depend on the distribution of
values of \pvsnr.

We will consider following alternatives: one where all \nstrat elements of \psnr are
equal (``all-equal''), and two where $m$ of \nstrat elements of \psnr
are equal to some positive value, and the remaining $\nstrat - m$ are negative
that value. For $m=1$, we call this the ``one-good'' alternative; for $m=\nstrat/2$,
the ``half-good'' alternative.

% power vs the MHT?
<<'mht_test_functions',echo=FALSE,eval=TRUE,cache=TRUE,dependson=c('motherload_sim_funcs')>>=
# set up the functions
bot_rawsim <- function(nday,nlatf,one_zeta,nsim=100,rho=0,
                       mutype=c('equal','onegood','halfgood','spread')) { 
	mutype <- match.arg(mutype)
  R <- pmin(diag(nlatf) + rho,1)  
  mu <- switch(mutype,
               equal=rep(one_zeta,nlatf),
               onegood=c(one_zeta,rep(-one_zeta,nlatf-1)),
               halfgood=c(rep(one_zeta,floor(nlatf/2)),
                          rep(-one_zeta,ceiling(nlatf/2))),
               spread=seq(-one_zeta,one_zeta,length.out=nlatf))
  sim <- parsim(nsim=nsim,nday=nday,R=R,mu=mu,testmu=rep(0,nlatf))
  # note that the MHT/bonferroni correction here is only valid
  # for rho==0.
  # see below for the proper version
  stopifnot(rho==0)

  # weights from chi-bar square distro
  cwts <- exp(lchoose(nlatf,0:nlatf) - nlatf * log(2))
  okwt <- (cwts > 1e-9)
  sbdf <- which(okwt) - 1
  sbwt <- cwts[okwt]
  # should check that sum(sbwt) is around 1
  stopifnot(abs(sum(sbwt) - 1) < 1e-5)
  cval <- 1/sqrt(1 + (nlatf-1) * rho)
  dval <- sqrt(1-rho)

  # MHT and chibarsquare
  mhtpvals <- replicate(nsim,{
    if (rho == 0) {
      x <- SharpeR::rsr(n=nlatf,df=nday-1,zeta=mu,ope=1)
    } else {
      X <- mvtnorm::rmvnorm(nday,mean=mu,sigma=R)
      x <- colMeans(X) / apply(X,2,sd)
    }
    xm <- which.max(x)
    # the nlatf is the MHT correction here, a Bonferroni hack.
    pval <- nlatf * SharpeR::psr(max(x),df=nday-1,zeta=0,ope=1,lower.tail=FALSE) 

    # one sided test
    zetavg <- mean(x)
    xi <- cval * zetavg + (x-zetavg) / dval

    # chibarsquare test, assuming rho=0
    chibs <- (nday) * sum(pmax(xi,0)^2)
    chibp <- sum(sbwt * pchisq(chibs,df=sbdf,lower.tail=FALSE))

    c(pval,chibp,mu[xm])
  })
  rbind(sim %>% 
        select(-simvals) %>% 
        mutate(pvals=1-pvals) %>% 
        mutate(method='condest'),
        data_frame(pvals=as.numeric(mhtpvals[1,]),
                   popvals=as.numeric(mhtpvals[3,]),
                   method='mht'),
        data_frame(pvals=as.numeric(mhtpvals[2,]),
                   popvals=as.numeric(mhtpvals[3,]),
                   method='chibs'))
}
mhtsim <- function(alpha=0.05,...) {
  bot_rawsim(...) %>%
    group_by(method,popvals) %>%
      summarize(rej_rate=mean(pvals < alpha),
                tot_base=n()) %>%
    ungroup() %>%
    arrange(method,popvals)
}
#set.seed(1234)
#mhtsim(nday=252*4,nlatf=100,one_zeta=0.10,nsim=200,mutype='spread')
#mhtsim(nday=252*4,nlatf=100,one_zeta=0.10,nsim=200,mutype='onegood')
#mhtsim(nday=252*4,nlatf=100,one_zeta=0.10,nsim=200,mutype='equal')
#mhtsim(nday=252*4,nlatf=100,one_zeta=0.10,nsim=200,mutype='halfgood')
#mhtsim(nday=252*4,nlatf=100,one_zeta=0.10,rho=0.7,nsim=200,mutype='onegood')
#mhtsim(nday=252*4,nlatf=200,one_zeta=0.10,nsim=200,mutype='halfgood')
#mhtsim(nday=252*4,nlatf=20,one_zeta=0.10,nsim=200,mutype='halfgood')
#mhtsim(nday=252*4,nlatf=10,one_zeta=0.10,rho=0.7,nsim=200,mutype='onegood')
#mhtsim(nday=252*4,nlatf=100,one_zeta=0.10,nsim=200,rho=0.99,mutype='equal')
#mhtsim(nday=252*4,nlatf=100,one_zeta=0,nsim=200,rho=0.99,mutype='equal')
#mhtsim(nday=252*4,nlatf=200,one_zeta=0,nsim=200,rho=0.99,mutype='equal')
#mhtsim(nday=252*4,nlatf=200,one_zeta=0,nsim=200,rho=0.999,mutype='equal')
@

<<'sims_vs_mht',echo=FALSE,eval=TRUE,cache=TRUE,dependson=c('mht_test_functions','runtime')>>=
suppressMessages({
  library(tidyr)
})

# actual run the sims
params <- data_frame(one_zeta=seq(0,0.15,length.out=13)) %>%
  tidyr::crossing(data_frame(mutype=c('onegood','halfgood','equal'))) 

mht_nsim <- ceiling(1e4/max(1,RUNTIME_PARAM))
mht_nday <- 4*252
mht_nlatf <- 100

registerDoFuture()
plan(multiprocess)
set.seed(1234)
#system.time({
resu <- params %>%
  group_by(one_zeta,mutype) %>%
    summarize(resu=list(mhtsim(nday=mht_nday,nlatf=mht_nlatf,one_zeta=one_zeta,nsim=mht_nsim,mutype=mutype))) %>%
  ungroup() %>%
  unnest()
#})
plan(sequential)
@

We compare the power of the conditional estimation procedure to that of a
simple MHT correction, and the one-sided test.
In our experiments, we 
draw returns from a Gaussian distribution with diagonal covariance.
Under this assumption, one can use the distribution of the \tstat{} statistic
to perform inference on the \txtSNR. \cite{pav_ssc}
We then use the Bonferroni correction to account for the multiple tests performed.
We also perform the one-sided test based on the chi-bar square statistic.

Note that in the all-equal case, since every asset has the same \txtSNR,
whichever we select will have the same \txtSNR, and the Bonferroni-corrected test should have the
same power as the \tstat{}-test for a single asset. 
The conditional estimation procedure, however, may suffer in this case as
we may condition on a \ssr[1] that is very close to being non-optimal,
resulting in a small test statistic for which we do not reject.
On the other hand, for the one-good case, as the $\nstrat - 1$ assets 
may have considerably negative \txtSNR, they are unlikely to exhibit the
largest \txtSR, and so the MHT is merely testing a single asset, but
at the $\typeI / \nstrat$ level instead of the $\typeI$ level, resulting
in lower power. The chi-bar square test is also unlikely to reject.
The conditional estimation procedure, however, should
not suffer under this alternative.

In fact, this is what we see. We perform simulations under all-equal, one-good,
and half-good configurations, letting the `good' \txtSNR vary from 
$0$ to $\Sexpr{max(params$one_zeta)}\dayto{-\halff}$, which corresponds
to an `annualized' \txtSNR of around
$\Sexpr{signif(sqrt(252) * max(params$one_zeta),2)}\yrto{-\halff}$.
We draw Gaussian returns with diagonal covariance for
$\Sexpr{mht_nlatf}$ assets, with $\ssiz=\Sexpr{mht_nday}$.
For each setting we perform $\Sexpr{mht_nsim}$ simulations then 
compute the empirical rejection rate of the test at the $0.05$ level,
conditional on the \txtSNR of the \emph{selected} asset, which is
to say the one with the largest \txtSR. Note that in some simulations
the largest \txtSR is observed in an asset with a negative \txtSNR.
We hope our tests to have lower power when this occurs.

In \figref{power_plotz_one}, we plot the power of the MHT Bonferroni
test, the chi-bar square test, 
and the conditional estimation procedure versus the \txtSNR
of the selected asset. 
We present facet columns for the three configurations, 
\viz all-equal, one-good, half-good.
A horizontal line at $0.05$ gives the nominal rate under the null,
which occurs as $x=0$ in these plots.
As expected from the above explanation, 
chi-bar square has the highest power for the all-equal alternative,
then the MHT, then the conditional estimation test.
These relationships are reversed for the one-good case.
The chi-bar square test has zero power against the one-good alternative.
All tests perform similarly in the half-good and all-equal alternatives.

<<'power_plotz_one',dependspn=c('sims_vs_mht'),echo=FALSE,eval=TRUE,cache=TRUE,fig.width=10,fig.height=5.5,out.width="0.975\\textwidth",fig.pos='h',fig.cap=fig_cap>>=
fig_cap <- paste0('The empirical power of the conditional estimation, MHT corrected test, and one-sided test are shown versus the \\txtSNR of the asset with maximum \\txtSR under different arrangements of the vector \\pvsnr.')
# 482b4da5-36ad-45fe-9c8f-687625aa5cc7 
#filter(grepl('equal|onegood',mutype)) %>%
library(ggplot2)
ph <- resu %>%
  mutate(method=case_when(grepl('^condest$',method)   ~ 'cond. est.',
                          grepl('^mht$',method)       ~ 'MHT',
                          grepl('^chibs$',method)     ~ 'chi-bar^2',
                          TRUE ~ 'ERROR')) %>%
  mutate(mutype=case_when(mutype=='equal'     ~ 'all-equal',
                          mutype=='halfgood'  ~ 'half-good',
                          mutype=='onegood'   ~ 'one-good',
                          TRUE ~ 'ERROR')) %>%
  ggplot(aes(popvals,rej_rate,color=method))  +
  geom_line() + geom_point() +
  facet_grid(.~mutype) + 
  geom_hline(yintercept=0.05,linetype=2,alpha=0.7) + 
  labs(x='signal noise ratio',
       y='rejection rate at nominal 0.05 level',
       color='method',
       title='power of tests')
print(ph)
@


The power of the conditional estimation procedure for the all-equal
case is rather disappointing. 
For the case where all assets have a \txtSNR of
$\Sexpr{signif(sqrt(252) * max(params$one_zeta),2)}\yrto{-\halff}$,
the test has a power of only around a half.
The test suffers from low power because we are conditioning on
``\ssr[1] is the largest \txtSR'', where we should actually
be conditioning on ``the asset with the largest \txtSR.''

Note the odd plot in the half-good facet: the MHT correction
and one-sided tests have greater
than $0.05$ rejection rate for \emph{negative} \txtSNR. 
The plot is somewhat misleading in this case, however.
We have performed
$\Sexpr{mht_nsim}$ simulations for each setting of the `good'
\txtSNR; in some very small number of them for the half-good
case, an asset with negative \txtSNR exhibits the maximum
\txtSR. 
We are plotting the rejection rate for the test in this case.
But note that the null hypothesis that MHT and the one-sided test are testing
\emph{is} violated in this case, because half the assets
have positive \txtSNR, and the alternative procedures test the null that all
assets have zero or lower \txtSNR.
We have not shown the probability that a `bad' asset has the
highest \txtSR, but note that when the `good' \txtSNR
is greater than $0.05\dayto{-\halff}$ we do not observe this
occuring even once over the $\Sexpr{mht_nsim}$ simulations performed
for each setting. 

We note that the one-sided test appears to have higher power than either
of the other tests for the all-equal and half-good populations,
but fails to reject at all in the one-good case, except under the null.
This is to be expected, since the chi-bar square test depends strongly
on all the observed \txtSRs, and in this case we expect many of them
to be negative.

We doubt that the simple experiments performed here
have revealed all the relevant differences between the various tests or when
one dominates the others.

\subsubsection{Simulations under the null versus with correlated returns}

<<'correlation_sims_vs_mht',echo=FALSE,eval=TRUE,cache=TRUE,dependson=c('motherload_sim_funcs')>>=
suppressMessages({
  library(tidyr)
})

# set up the functions
rawsim <- function(nday,nlatf,nsim=100,rho=0) {
  R <- pmin(diag(nlatf) + rho,1)  
  mu <- rep(0,nlatf)

  sim <- parsim(nsim=nsim,nday=nday,R=R,mu=mu,testmu=rep(0,nlatf))

	apart <- sqrt(nday)/sqrt(1-rho)
	bpart <- sqrt(nday) * ((1/sqrt(1-rho+nlatf*rho)) - (1/sqrt(1-rho)))

  # weights from chi-bar square distro
  cwts <- exp(lchoose(nlatf,0:nlatf) - nlatf * log(2))
  okwt <- (cwts > 1e-9)
  sbdf <- which(okwt) - 1
  sbwt <- cwts[okwt]
  # should check that sum(sbwt) is around 1
  stopifnot(abs(sum(sbwt) - 1) < 1e-5)
  cval <- 1/sqrt(1 + (nlatf-1) * rho)
  dval <- sqrt(1-rho)

  mhtpvals <- replicate(nsim,{
		X <- mvtnorm::rmvnorm(nday,mean=mu,sigma=R)
		x <- colMeans(X) / apply(X,2,sd)
		bonf_pval <- nlatf * SharpeR::psr(max(x),df=nday-1,zeta=0,ope=1,lower.tail=FALSE) 
		# do the correction
		corr_stat <- apart * max(x) + bpart * mean(x)
		corr_pval <- nlatf * pnorm(corr_stat,lower.tail=FALSE)

    # one-sided test
    zetavg <- mean(x)
    xi <- cval * zetavg + (x-zetavg) / dval
    # chibarsquare test, assuming rho=0
    chibs <- (nday) * sum(pmax(xi,0)^2)
    chibp <- sum(sbwt * pchisq(chibs,df=sbdf,lower.tail=FALSE))

    c(bonf_pval,corr_pval,chibp)
  })
  data_frame(bonf_pvals=as.numeric(mhtpvals[1,]),
						 corr_pvals=as.numeric(mhtpvals[2,]),
             chib_pvals=as.numeric(mhtpvals[3,]),
             nelc_pvals=1-sim$pvals)
}
many_rawsim <- function(nday,nlatf,rho,nsim=1000L,nnodes=7) {
  if ((nsim > 10*nnodes) && require(doFuture)) {
		registerDoFuture()
		plan(multiprocess)
		nper <- as.numeric(table(1:nsim %% nnodes))
    retv <- foreach(iii=1:nnodes,.export = c('nday','nlatf','rho','nper','rawsim')) %dopar% {
			rawsim(nday=nday,nlatf=nlatf,rho=rho,nsim=nper[iii])
    } %>%
      bind_rows()
  } else {
		retv <- rawsim(nday=nday,nlatf=nlatf,rho=rho,nsim=nsim)
  }
  retv
}
mhtsim <- function(alpha=0.05,...) {
  many_rawsim(...) %>%
		tidyr::gather(key=method,value=pvalues) %>%
    group_by(method) %>%
      summarize(rej_rate=mean(pvalues < alpha)) %>%
    ungroup() %>%
    arrange(method)
}


# actual run the sims
params <- data_frame(rho=seq(0,0.9,by=0.1)) 

options(future.globals.maxSize=1e9)
mht_null_nsim <- ceiling(5e3/max(1,RUNTIME_PARAM))
mht_nday <- 4*252
mht_nlatf <- 100

registerDoFuture()
set.seed(4321)
#system.time({
rhoresu <- params %>%
  group_by(rho) %>%
    summarize(resu=list(mhtsim(nday=mht_nday,nlatf=mht_nlatf,rho=rho,nsim=mht_null_nsim))) %>%
  ungroup() %>%
  unnest()
#})
@

On the other hand, the MHT test cannot maintain the nominal type I rate in the face of correlated assets.
To demonstrate this, we repeat the experiments above, but setting 
$\RMAT=\makerho{\rho}{\wrapParens{1-\rho}}$
and $\psnr=\vzero$.
Again we consider $\nstrat=\Sexpr{mht_nlatf}$, 
$\ssiz=\Sexpr{mht_nday}$, and perform $\Sexpr{mht_null_nsim}$ simulations to estimate the 
empirical rejection rate.
In \figref{rho_plotz_one}, we plot the empirical rejection rate versus $\rho$ at the nominal
$0.05$ type I level. 
While the conditional estimation procedure and one-sided tests appear to maintain the nominal rejection
rate, even though \RMAT is being estimated from the sample,
the MHT test is conservative, with near zero rejection rates for large $\rho$.
The fix for common correlation described in \subsecref{fix_bonferroni} is also tested,
yielding nominal rejection rates.
%If \svsr were normally distributed, which is a fair approximation, this would be guaranteed
%by Slepian's Lemma, which tells us that for a normally distributed Gaussian vector
%with fixed mean and variance, the maximum element is `stochastically decreasing' in the correlation.  \cite{slepian1962one}
%Intuitively, the number of `pseudo assets' is reduced when returns have higher correlation, and
%the Bonferroni correction is too conservative.

<<'rho_plotz_one',dependson=c('correlation_sims_vs_mht'),echo=FALSE,eval=TRUE,cache=TRUE,fig.width=10,fig.height=5.5,out.width="0.975\\textwidth",fig.pos='h',fig.cap=fig_cap>>=
fig_cap <- paste0('The empirical type I rate under the null hypothesis is plotted versus $\\rho$ ',
                  'for the case where $\\RMAT=\\makerho{\\rho}{\\wrapParens{1-\\rho}}$, ',
                  'for four testing procedures: the conditional estimation, the chi-bar square test, the vanilla Bonferroni correction, ',
                  'and Bonferroni correction with fix for common correlation.  ',
                  'Tests are performed with Gaussian returns, for ',mht_nlatf,' assets over ',mht_nday,' days. ',
                  'Tests were performed at the $0.05$ level, which appears to be maintained by all procedures except the ',
                  'regular Bonferroni procedure.  ',
                  'Empirical rates are over ',mht_null_nsim,' simulations. The $y$ axis is in log scale.')
library(ggplot2)
ph <- rhoresu %>%
  mutate(method=case_when(grepl('^nelc_pvals$',method)    ~ 'cond. est.',
                          grepl('^bonf_pvals$',method)    ~ 'vanilla MHT',
                          grepl('^chib_pvals$',method)    ~ 'chi-bar^2',
                          grepl('^corr_pvals$',method)    ~ 'fixed MHT',
                          TRUE ~ 'ERROR')) %>%
  ggplot(aes(rho,rej_rate,color=method))  +
  geom_line() + geom_point() +
  geom_hline(yintercept=0.05,linetype=2,alpha=0.7) + 
  scale_y_log10() + 
  labs(x=expression(rho),
       y='rejection rate at nominal 0.05 level',
       color='method',
       title='rejection rate of tests under the null')
print(ph)
@

%UNFOLD

\clearpage

\subsection{Five Industry Portfolios}%FOLDUP

% 2FIX: start from here ... 
<<'load_mind_data',echo=FALSE,eval=TRUE,cache=TRUE>>=
if (!require(aqfb.data) && require(devtools)) {
  stop('please install_githb("shabbychef/aqfb_data")')
  #devtools::install_github("shabbychef/aqfb_data")
}
library(aqfb.data)
data(mind5)
svsr <- colMeans(mind5) / apply(mind5,2,sd)
yord <- order(svsr,decreasing=TRUE)
X <- mind5[,yord]
svsr <- colMeans(X) / apply(X,2,sd)
R <- cov2cor(cov(X))
medrho <- median(R[row(R) < col(R)])
ssiz <- nrow(X)
nlatf <- ncol(X)
TEO <- time(mind5)
TEO_0 <- TEO[1]
TEO_f <- TEO[length(TEO)]

library(SharpeR)
pv <- psr(svsr[1],df=ssiz-1,ope=1,lower.tail=FALSE)
bestind <- colnames(X)[1]
@

We download the monthly returns of five industry portfolios
from Ken French's data library. \cite{ind_5_def}
%These consist of 
We consider the \Sexpr{ssiz} months of data on five industries
from \Sexpr{TEO_0} to \Sexpr{TEO_f}.
We compute the \txtSR of the returns of each, and present them
in \tabref{mind_sr}. 
We have reordered the industries in decreasing \txtSR.
The industry portfolio with the highest \txtSR was
\Sexpr{bestind} with a \txtSR of 
around $\Sexpr{svsr[1]}\,\moto{-\halff}$ which is approximately
$\Sexpr{sqrt(12)*svsr[1]}\,\yrto{-\halff}$.

<<'mind_sr',echo=FALSE,cache=TRUE,results='asis'>>=
foo_tab <- data_frame(industry=names(X),sr=as.numeric(svsr)) %>%
  mutate(sr=sprintf('$ %.3f\\,\\moto{-\\halff} $',sr)) %>%
  rename(`Sharpe Ratio`=sr) 
library(xtable)
xres <- xtable(foo_tab,
	digits=c(3,3,3),
	label="tab:mind_sr",
	align=c("c","r|","c"),
	caption=paste0("The \\txtSRs of the five industry portfolios are shown."))
print(xres,sanitize.text.function=identity,include.rownames=FALSE)
@

<<'mind_inference',echo=FALSE,cache=TRUE,results='asis'>>=
options(width=64,digits=3)
typeI <- 0.05
library(SharpeR)
cis <- confint(as.sr(X,epoch=12),level.lo=typeI,level.hi=1)
exacty <- cis[1,1]

hsr <- svsr[1]
se_smal <- sqrt((1/ssiz) * 1)
se_big <- sqrt((1/ssiz) * (1 + 0.5 * hsr^2))
silly <- hsr + qnorm(typeI) * se_big
bonf_one <- hsr + qnorm(typeI/nlatf) * se_big

allrho <- (R[row(R) < col(R)])

abit <- 1 / sqrt(1-medrho)
bbit <- (sqrt(ssiz)) * ((1/sqrt(1-medrho+medrho*nlatf)) - (1/sqrt(1-medrho)))
zavg <- mean(svsr)
tgtval <- qnorm(typeI/nlatf)
bonf_two <- - (tgtval - abit * hsr - bbit * zavg) / bbit

Rhat <- R
mySigma <- (1/ssiz) * (Rhat + (1/2) * diag(svsr) %*% (Rhat * Rhat) %*% diag(svsr))
nu <- c(1,rep(0,nlatf-1))
AAA <- cbind(-1,diag(nlatf-1))
bbb <- rep(0,nlatf-1)
#conde <- citn(1-typeI,svsr,AAA,bbb,nu,mySigma) 
library(epsiwal)
conde <- epsiwal::ci_connorm(y=svsr,
                             A=AAA,b=bbb,
                             eta=nu,Sigma=mySigma,p=1-typeI)
#checky <- ptn(svsr,AAA,bbb,nu,mu=NULL,Sigma=mySigma,numu=conde)

# now the chi-bar square procedure

#' @param x  a vector of observed Sharpe ratios, in daily units.
#' @param nday  the number of days to compute the SR.
pchibsq <- function(x,nday,zeta=0,rho=0,lower.tail=TRUE) {
  nlatf <- length(x)

  # weights from chi-bar square distro
  cwts <- exp(lchoose(nlatf,0:nlatf) - nlatf * log(2))
  okwt <- (cwts > 1e-9)
  sbdf <- which(okwt) - 1
  sbwt <- cwts[okwt]
  # should check that sum(sbwt) is around 1
  stopifnot(abs(sum(sbwt) - 1) < 1e-5)
  cval <- 1/sqrt(1 + (nlatf-1) * rho)
  dval <- sqrt(1-rho)

  # one-sided test
  zetavg <- mean(x)
  xi <- cval * zetavg + (x-zetavg) / dval
  # chibarsquare test, assuming rho=0
  chibs <- (nday) * sum(pmax(xi - cval * zeta,0)^2)
  chibp <- sum(sbwt * pchisq(chibs,df=sbdf,lower.tail=lower.tail))
}

ci_chibsq <- function(x,nday,rho=0,typeI=0.05,lower.tail=FALSE) {
  zerme <- function(zeta) { pchibsq(x=x,nday=nday,zeta=zeta,rho=rho,lower.tail=lower.tail) - typeI }
  fval <- uniroot(zerme,
                  interval=c(min(min(x),-3),max(x)),
                  maxiter=2000)$root
}
os_cilo <- ci_chibsq(svsr,nday=ssiz,typeI=typeI,lower.tail=FALSE)
@

We are interested in computing $\Sexpr{100 * (1-typeI)}\%$ upper confidence
intervals on the \txtSNR of the \Sexpr{bestind} portfolio. 
We are only considering this portfolio as it is the one with maximum \txtSR
in our sample.
If we had been interested in testing
\Sexpr{bestind} 
without our conditional selection, we would compute the confidence interval
$\left[\Sexpr{exacty}\,\moto{-\halff}, \infty\right)$ based on inverting the non-central
\tstat{}-distribution. \cite{pav_ssc,SharpeR-Manual}
If instead we approximate the standard error by plugging in 
$\Sexpr{hsr}\,\moto{-\halff}$ as the \txtSNR of 
\Sexpr{bestind} 
into \eqnref{apx_srdist_gaussian},
we estimate the standard error of the \txtSR to be
$\Sexpr{se_big}\,\moto{-\halff}$.
Based on this we can compute the na\"{i}ve confidence interval of
the measured \txtSR plus 
$\qnorm{\Sexpr{typeI}}=\Sexpr{qnorm(typeI)}$ times the standard error.
This also gives the confidence interval
$\left[\Sexpr{silly}\,\moto{-\halff}, \infty\right)$.

Using the simple Bonferroni correction, however, since we selected
\Sexpr{bestind} 
only for having the maximum \txtSR, we should compute the 
confidence interval by adding
$\qnorm{\Sexpr{typeI/nlatf}}=\Sexpr{qnorm(typeI/nlatf)}$ times the standard error.
This yields the confidence interval
$\left[\Sexpr{bonf_one}\,\moto{-\halff}, \infty\right)$.

The correlation of industry returns is high, however.
The pairwise sample correlations range from
\Sexpr{min(allrho)} to 
\Sexpr{max(allrho)} with a median value of
\Sexpr{medrho}.
Plugging this value in as $\rho$, 
we find the value $c$ such that the $z_1$ from \eqnref{fix_bonf_z}
is equal to 
$\qnorm{\Sexpr{typeI/nlatf}}=\Sexpr{qnorm(typeI/nlatf)}$.
This leads to the confidence interval 
$\left[\Sexpr{bonf_two}\,\moto{-\halff}, \infty\right)$.

We use this estimate of $\rho$ to compute the chi-bar square test.
We invert the test to find the $\Sexpr{100 * (1-typeI)}\%$ upper confidence interval
$\left[\Sexpr{os_cilo}\,\moto{-\halff}, \infty\right)$.


Finally we use the conditional estimation procedure, inverting
the hypothesis test to find the corresponding population value.
This yields the confidence interval 
$\left[\Sexpr{conde}\,\moto{-\halff}, \infty\right)$.
%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work}%FOLDUP

The conditional estimation procedure appears to achieve nominal
type I rates under the null, and does not seem unduly harmed by assuming
the vector \svsr is normally distributed. 
Nor does it seem to suffer greatly from using sample estimates
of the correlation matrix, \RMAT, nor from the presence of 
kurtotic returns.
The procedure can be used for other test configurations beyond the
asset with the maximum \txtSR,
and can be used to construct confidence intervals.
However, it appears to have low power when many assets have the
same \txtSNR.

Clearly the low power of the test gives us reason to seek improvements.
Perhaps the conditioning procedure can be adapted to recognize
that the strategist would have been testing another asset if the
\txtSR of the currently selected asset had been lower. 
Perhaps the power of the MHT, or the chi-bar square test for the case of equal \txtSNRs can
be somehow ported to the conditional estimation procedure, maybe
by testing multiple 
assets\footnote{Note that White's Reality Check and Hansen's SPA
appear to avoid this issue by comparing the maximum in-sample 
predictive ability to the \emph{average} predictive ability of
tested models.  \cite{White:2000,Hansen:2005}}.
Finally, the fix for Bonferroni correction under rank-one updated
correlation \RMAT can potentially be generalized to deal
with more realistic correlation matrices.

%Lastly we mention that when asset returns are well approximated
%by a linear combination of $m$ `latent' returns, we can
%view the asset with maximum \txtSR as holding the sample \txtMP
%over the latent returns, then appeal to the theory of the
%statistic of the \txtMP via Hotelling's $T^2$ statistic.  \cite{pav_maxsharpe_two}
%This approach requires further development.
%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% bibliography%FOLDUP
%\nocite{markowitz1952portfolio,markowitz1999early,markowitz2012foundations}
%\bibliographystyle{jss}
%\bibliographystyle{siam}
%\bibliographystyle{ieeetr}
\bibliographystyle{plainnat}
%\bibliographystyle{acm}
%\bibliography{SharpeR,rauto}
\bibliography{common}
%\bibliography{AsymptoticMarkowitz}
%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix%FOLDUP

\section{Establishing \eqnref{apx_srdist_elliptical}.}


Let \Mtx{V} be the variance covariance to be computed. Then from
\eqnref{delmethod}, 
\begin{align*}
  \Mtx{V} &= \qoform{\pvvar}{\wrapParens{\dbyd{\pvsnr}{\vcat{\pvmu}{\pvmom[2]}}}},\\
   &= 
   \Mdiag{\frac{1}{\pvsigma^2}}
\onebytwo{\Mdiag{\pvsigma + \pvmu\pvsnr}}{
  {\Mdiag{\frac{- \pvsnr}{2}}}} 
  \pvvar
\twobyone{\Mdiag{\pvsigma + \pvmu\pvsnr}}{
  {\Mdiag{\frac{- \pvsnr}{2}}}} 
   \Mdiag{\frac{1}{\pvsigma^2}}.
\end{align*}

Plugging in \pvvar from \eqnref{elliptical_variances}, we have
\begin{align*}
  \Mtx{V} &= 
   \Mdiag{\frac{1}{\pvsigma^2}}
   \left\{
\Mdiag{\pvsigma + \pvmu\pvsnr}
\pvsig
\Mdiag{\pvsigma + \pvmu\pvsnr}
+ 
\Mdiag{\pvsigma + \pvmu\pvsnr}
2\pvsig\Mdiag{\pvmu}
\Mdiag{\frac{- \pvsnr}{2}}
   \right.\\
   &\phantom{=\Mdiag{\frac{1}{\pvsigma^2}}}\,
   \left.
+\Mdiag{\frac{- \pvsnr}{2}}
2\Mdiag{\pvmu}\pvsig
\Mdiag{\pvsigma + \pvmu\pvsnr}
+
\Mdiag{\frac{- \pvsnr}{2}}
2 \pvsig \hadm \pvsig
\Mdiag{\frac{- \pvsnr}{2}}\right.\\
   &\phantom{=\Mdiag{\frac{1}{\pvsigma^2}}}\,
   \left.
+
\Mdiag{\frac{- \pvsnr}{2}}
	4 \Mdiag{\pvmu}\pvsig\Mdiag{\pvmu}
\Mdiag{\frac{- \pvsnr}{2}}
   \right\}
   \Mdiag{\frac{1}{\pvsigma^2}},\\
  &= 
   \Mdiag{\frac{1}{\pvsigma^2}}
   \left\{
 \Mdiag{\pvsigma}
\pvsig
  \Mdiag{\pvsigma}
-
\Mdiag{\pvmu\pvsnr}
\pvsig
\Mdiag{\pvmu\pvsnr}\right.\\
   &\phantom{=\Mdiag{\frac{1}{\pvsigma^2}}}\,
   \left.
+
\Mdiag{\frac{- \pvsnr}{2}}
2 \pvsig \hadm \pvsig
\Mdiag{\frac{- \pvsnr}{2}}\right.\\
   &\phantom{=\Mdiag{\frac{1}{\pvsigma^2}}}\,
   \left.
+
\Mdiag{\pvmu\pvsnr}
\pvsig
\Mdiag{\pvmu\pvsnr}
   \right\}
   \Mdiag{\frac{1}{\pvsigma^2}},\\
&=
   \Mdiag{\frac{1}{\pvsigma^2}}
   \wrapBraces{
 \Mdiag{\pvsigma}
\pvsig
  \Mdiag{\pvsigma}
  + \half
\Mdiag{\pvsnr}
\pvsig \hadm \pvsig
\Mdiag{\pvsnr}}
   \Mdiag{\frac{1}{\pvsigma^2}},\\
&=
\RMAT 
+ \half
\Mdiag{\pvsnr}
   \Mdiag{\frac{1}{\pvsigma^2}}
\pvsig \hadm \pvsig
   \Mdiag{\frac{1}{\pvsigma^2}}
\Mdiag{\pvsnr},\\
&=
\RMAT 
+ \half
\Mdiag{\pvsnr}
\RMAT \hadm \RMAT
   \Mdiag{\frac{1}{\pvsigma^2}}.
\end{align*}
%UNFOLD

Proving \eqnref{apx_srdist_elliptical} is similar, and is left as an exercise for the reader.

\end{document}
%for vim modeline: (do not edit)
% vim:fdm=marker:fmr=FOLDUP,UNFOLD:cms=%%s:syn=rnoweb:ft=rnoweb:et:nu:tw=151:cole=0:fo=croql
